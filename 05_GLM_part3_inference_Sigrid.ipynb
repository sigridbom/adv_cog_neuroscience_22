{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The GLM, part 3: inference\n",
    "In this notebook, we'll continue with the GLM, focusing on statistical tests (i.e., inference) of parameters.\n",
    "\n",
    "In the last tutorial, you learned how to estimate parameters of the GLM and how to interpret them. This week, we'll focus on statistical inference of those estimated parameters (and design of experiment, in another notebook). Importantly, we are going to introduce the most important formula in the context of univariate fMRI analyses: the formula for the *t-value*. Make sure you understand this formula, as we will continue to discuss it in the next weeks.\n",
    "\n",
    "**What you'll learn**: after this tutorial ... \n",
    "* you know how the different parts of the t-value formula and how they relate to your data and experiment;\n",
    "* you are able to calculate t-values and corresponding p-value of parameters from a GLM\n",
    "\n",
    "This tutorial is based on the online textbook here:\n",
    "https://lukas-snoek.com/NI-edu/fMRI-introduction/week_3/glm_part2_inference.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/coder/.local/lib/python3.9/site-packages (1.23.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/coder/.local/lib/python3.9/site-packages (3.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/coder/.local/lib/python3.9/site-packages (from matplotlib) (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/coder/.local/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/coder/.local/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/coder/.local/lib/python3.9/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/coder/.local/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/coder/.local/lib/python3.9/site-packages (from matplotlib) (1.23.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/coder/.local/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/coder/.local/lib/python3.9/site-packages (from matplotlib) (4.34.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/coder/.local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/coder/.local/lib/python3.9/site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /home/coder/.local/lib/python3.9/site-packages (from scipy) (1.23.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# First some imports\n",
    "\n",
    "import os\n",
    "import pip\n",
    "os.system('python3 -m pip install numpy')\n",
    "os.system('python3 -m pip install matplotlib')\n",
    "os.system('python3 -m pip install scipy')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "From your statistics classes, you might remember that many software packages (e.g. SPSS, R, SAS) do not only return beta-parameters of linear regression models, but also <b>*t*-values</b> and <b>*p*-values</b> associated with those parameters. Like beta-parameters, these statistics evaluate whether a beta-parameter (or combination of beta-parameters) differs significantly from 0 (or in fMRI terms: whether a voxel activates/deactivates significantly in response to one or more experimental factors).\n",
    "\n",
    "In univariate (activation-based) fMRI studies, we need statistics to evaluate the estimated parameters in context of the *uncertainty* of their estimation. As we'll discuss later in more detail, interpreting (and performing inference about) the magnitude of GLM parameters without their associated uncertainty is rarely warranted in univariate fMRI studies. To illustrate the problem with this, let's look at an example.\n",
    "\n",
    "In this example, we try to predict someone's height (in meters; $\\mathbf{y}$) using someone's weight (in kilos; $\\mathbf{X}$). (Note that the data is not necessarily representative of the true relationship between height and weight.)\n",
    "\n",
    "Anyway, let's run a linear regression using weight (in kilos) as a predictor for height (in meters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAGhCAYAAADRFZiVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABwk0lEQVR4nO3dd3xUVfrH8c9DkyAlIAqCBSyABQXF3igqqKsgVmxgXesquqxlLai7gqv+7OtaVrFjR6yoQGQVK0VBFAVRMIANgwKhJef3x7lJJmFqMsncmXzfrxevIffeufPM3Jm5z5znnHPNOYeIiIiIhE+DTAcgIiIiItEpURMREREJKSVqIiIiIiGlRE1EREQkpJSoiYiIiISUEjURERGRkFKilsPMrMDMan3+FTNzZlZQ24+TLDPrFMQ0JtOxSO1J53E2s+/M7LuaRxV+ZjYyeN16J7n9mGD7TmGJKc5+egf7GZmWwJJ/3HfNbJaZNYhYNiyIZVhdxiJg3mdm9r9Mx5IOStQyJPgAR/4rMbNlQXI1zMws0zGWqU8nsVSELUGVuqMfA7mjpkmimR0LHAhc55wrTWdsYVdXjQGpcn6C2GuB/YPjk9UaZToA4frgtjGwHXA0cBDQC7gwU0GlaAdgVaaDkHqnEP/eW57pQKTG7gHGAgszHUgqgh/U/wS+Bl6qsvol4ENgSV3HJeCce9nMvgT+aWYvuCye3V+JWoY550ZG/m1m+wFTgPPN7Dbn3IKMBJYC59xXmY5B6h/n3DpA770c4Jz7Bfgl03FUw8FAF+DvVRMB59xy9CMi0x4FRgP9gHcyHEu1qfQZMs659/EnHwN2r7rezPYys+fNbKmZrTWzRWZ2v5l1SGb/ZtbEzC40s9fN7HszWxOUXN8xs8OqbNs7aNbeGti6Sql2TMR2UUuAZtbKzEaZ2VwzW21mv5nZBDM7OMq25X1LzKyHmb1mZkVmtiro/7FvMs8vyn67mdm44DmuNLP3zOzQONsPMbPJwWOvNrMvzexqM9soYpthEc39B1V5XUaaWfPg2LxfZd95wT6dmZ1aZd15wfIzqixvE7yGX5pZsZktN7OJNX0OEdu6oHzR1sweMLMlwXviCzM7PcHLG7mfD4LnvHGV5e8Gj/HfKst3CJY/VmV5IzM738w+NLPfg+M/I3jPNqiybczyo5l1MbMXgvfcSjObamZHWIJ+Q2a2sZndYmYLg9dhnpldblbRFcF8/6eyH1BDqxz/qPut8hh9gtd6TvAci81stpldZ2ZNo2xfXpozs2PN7OPgdVlmZmPNrGOMx9ndzN40sz+Cx3nHzPZJFF+C2P9svi/WajP7MXgerWJsu4WZ3WNm3wav5a9mNt7M9oj3HKOsO9nMpgev009m9riZdbAEZTdL4nvEfJeO64I/J0ceyyRfkjOD22eiPH7U95oFXUmSea8lUvYamFljM7vWzOYHx2aumZ0dsd25wXErNrMfzOz6qp+niG0TnmPKPnv46k/VrjwFVfZXrfeBmZ1kZh+Z2QqL6HpjZkeZ/w4s+65aHBzb86M8nbHB7ZlR1mUNtaiF27rIP8yfxB8A1gDjgUXA9sBZwJFmtrdzLlHpoA1wJzAVeBv4GdgcOBJ43czOds49FGz7Hb40e0nw9x0R+5kZ70HMLB94H9gR+CS4b1vgeOAtMzvPOXd/lLv2Av4GfAA8BGwFHANMNLMezrm5CZ5fpM7BfmYB9+Of5wnAG2Z2knOu0permT0MnA78ALwAFAF7AzcC/czsEOfc+uC5X4//gv8eGBOxmwLn3Aoz+xjYy8xaOOf+CNbtB5QlS/2AxyPu1y+4nRgRz9ZAAdAJ+B/wJrAx8CfgTTP7s3PuwWo+h0j5+GO1Fng+iPE44GEzK3XOPUpiE4PHOSCIEzNrFiyLfH7xnm9j4BWgPzAXeApYDfQB7gb2Ak4lATPrhn9/twZeAz4HtsGXol6Pc9fGwASgA/AGsB4YhP9F3pSKbgoF+NfsYuAzYFzEPmYmig+4HCiL8bVg3/sBI4HeZnawc64kyv3OB47Cf/bfxb8eJwC7Bp+NNWUbBgnJO0AT4EVgHtAjiH1SEjFG8y/8sXkFeAt/XM7Gd9noG7mhme0WbNMG/5q+iP/8DwLeM7OjnXPxjkXZfv4G3Az8hm8dWQ4cgn+/xmutSvZ75I4gpoOC/X+XKKaI2Az/vJc65+Yne79Asu+1ZI3Fvx9ex583jgUeMLN1wC7AUOBV/OftKHz/rVX41zbyOSV7jikKYhyG/yEfGe93Efur7vvgMvxxfgWYDLQK9ncO/rt8abDuF2Cz4DmeDvw7cifOue/NrBA42Mwsa8ufzjn9y8A/wBH0eayy/ECgBP9B2TxieRf8iXQe0LHKffoF93mpyvKCqo+BPwlvEeVxWwGzgWVAXpV13wHfJXguBVWW3R8svx+wiOXb479g1wCdIpb3LntNgGFV9vXnYPm/k3xtO0Xs65Yq63rhv8h+A1pGLB8WbP9ilOc/Mlh3caLnHbHuhmD9ERHLRuG/kCcCiyKWNwB+BeZHOX6lwIlVlufjE4JioF0anoPDn8waRizfMYh1TpKved+qrzf+pO7wX9QO2DZi3UvBsi2jxHh3lVgaAv8N1g2McpzHVIllYrD8vCrLD4vzHvsuWP565GuHPwkUBf8aJ3rsJF+rbYj4TEQsvzHY5wkxjt3vQPcq654K1h0fsczwrfKVXq9g3cURr0HvJOMdE2y/ENgqYnkjfDcNB+xZZfk8fJJ9UJV9dcD3LVwCbBTlOfaOWLYN/rP6c5X3iQFPlz2PKvvvHecYR/0eifbYSb4u3YL7vRJj/bB0vNcSxFAQ7OsTIL/Ka7cW/z23gIhzBv7745fgdW0UsTwt55g0vQ9WAj2j7HMa/tyxWZR1bWPEUfZds2MqxzdM/zIeQH39F/FlMjL490988/la/Mn5oirb306VE3+V9S/hT6wtIpbF/BDF2MelwWMcWGX5d6SQqOF/xa8E/gDaRNm+7IR0bcSy3sGy96Js3xj/hf1pks+jU7CvosjXI2L9mGD90IhlM4LHyI+yfcPgi+3jeM+7yrqDgvX/F7HsY+Aj4IJgXZdg+W7B3w9EbLtrsOy5GPsfGKw/Pw3PYSURSWvEuneD9c2TeM2b4hPH6RHL/hXEs0+wn3OC5Q3wJ5CvI7YtS1aXEHHyiFifH3wuno1ynMdELNsyWPYN0CDKft4m/slzuyj3eTRYt3O8x67pP3yrgwMerrJ8ZLD8H1Hu0ydYd2vEsv2CZe/GeB/Mo3qJ2llR1p0erLswynvzlhj7uzhYf3iU59g7YtnVVPmeiFi3Nf77zlVZ3psUv0eiPXaSr8uhVPncVlk/LB3vtQQxFATb94uyblKw7owo6x4J1m0dsSyt55gavg9uj3Gfafjvq9YpHKf7gn0OSOX4humfSp+Zd12Vvx1wpnPukSrLy/qWHBStto//NdYQ/6toWrwHNLOdgBH41rvN8SfZSFH7vKSgK9AMeN85tyzK+kn4L+GeUdZ9WnWBc26dmf2IL2WlYrqrKDtGKsCXAnoCjwYlul3xicwlMbqIrMGPMEzWB/jEpR/4/nr4hOxfVJSe+uFHi5WVjSJLUmXHu5VFnxNq0+B2h2D/NXkO3zjnfo+yfFFw2xpYEW2HZZxzq81sKtDHzDZxzv2Kf16fOOc+CI5fP3xZZTd84hVZeu6CT1S+Aa6OEX9xjPgj9QhuP3DRp0p4D98BPJrlzrl5UZZHvg41Zr4f38X4Ed5dgBb4VqIysT5/G3w2YsS2W3D7btWNnXMlZvYesG0qMaf4+GXv3a1jvHe3D253IH4puuz74b2qK5wvaS3CJ8xJxVqD75FYNgluf6vGfdP9Xot2bBYHt9HOB4XB7Rb47huQxnNMlf1V533wcYx9PgncBswxs7H49/j7zrmf48RRdg5qmzDikFKilmHOOYPyL+998CWe/5jZ9865yBN32ZfCiAS7bB5vpZntjU8IGuFLROPxJZVS/EluIBX9qKqrVXAba1h62fL8KOuKYtxnPf5LIhU/xli+NLgti7M1/kS5KRsmztXinFsbnBAPNrNNgX3x8U90zn1pZkvwict9wa2jcqJWdrwPCf7FUna8a/IcimIsL+vLluzrPhGfnPUxs4n4E+1NwbpJBP1EiNI/jYrnuz3x44/7/qbimMY69rGWQ/peh5iCfniTgD3xXQ2ewZegyvqjXkfsz1+0+KLFlug1WBpjeSLJPn7ZsTwuwf7ScSw7xVhXFGN5db5HYikObjcYAJKEohjLq/Vec36Eaax9xVvXOGJZWs4xUfZXnfdB1Peoc+7/zOwXfH/Nv+D7TzszexcY4ZyLlrDmBbfFUdZlBSVqIeGcWwm8Y2ZHAtPxLT1dnXNl85OVfdhaxWj9SNbV+DduH+dcQeQKM7sSn6jVVFms7WOs37zKdrWlXYzlZXEtr3I7wzm3W5Ttq2sSPsnqh0/UVuM7QZetO8z8SMwDgC+ccz9F3Lcspoudc3cl8Vi19RxSUZZoHoxP/BtQkYxNAobgW/3KEtPJEfcti/8l59zgGsRQ9tmIdexjLa8rA/FJ2hjnXKVRtWa2Oen5oVD2WiZ6/9eWsscf6JwbX4P9RB7LL6Ksz/SxLPu8bhJ3q+yRrnNM1f1V533gYq5w7jHgsWDA2r74lukzgAlm1i1K61rZ8fmJLKXpOULGOfc58CC+SXp4xKoPg9sDavgQ2wHLqiZpgYNi3KeE1H7hzcWPKNo1+DBV1Se4nZ7CPqtjNzNrEWV57+B2BoBzbgX+RLCTmbVJYf+lxH9dypKUfviWpqnOudUR69oA5+FHck6sct+UjncNnkM6fYI/uZY932J8CRgqnt/h+D5Unzs/d1aZrwhGqAatTtU1M7jdJ8b0A/vXYN+RykZlpto6s11w+2KUdbE+f6kq+1xtsD8za0j6XoNY0vVdNSO43SDeYET0ljXcf5nqHssvgvt2S1McmVad41YC5e+rdOwvac65Iufc6865s/H9KNvgu/NU1Q3/XT2rNuKoC0rUwukf+P5EfzWzsr4K9+DLI7ebWZeqdzA/P1oyH4jvgDZmtkuV+5+JH6UXza/ApmaWF2N9Jc65tfi+BC3wAwciH2dbfJP1OipPT1EbWuGHoUc+fi/gZPyvvZciVv0ffhDEw9GSSzNrHQw1j/Qr8U8W04PHGQjsROVkrKz16coqfwMQNOH/DxhsVeZWi4ipu5ltVsPnkDbOTykxBZ+MHIfv0L0mWLcA/967GN9/serzXY8f7bk5cFe095qZbW5mOyaIYSG+D+J2+FF+kfcfQOz+aan6Df+rf6sU7/ddcNs7cqGZbUOVqRJqYCr+x9KBZla1hfxCqtc/LRUvA/OBC8zs8GgbmNk+Qb/KeJ7Cl+guMrPyz1lQPh9F+kqYvwa3KR3LoNw4E9gl2e/GkKvOOSbea5eu90Hk9n0segfWsu/BVVW23wjfpWeGc64o2ccJG5U+Q8g5V2hm/8Gf1P4GXOmc+yo4YT8MfGFmb+I7ojfGf0gOwPd1SfTr7g58QvaemT2LTyR64X+1Po+ff6eqicAe+Lm7puCTyM+cc6/EeZwrgpguDDqmTqZiHrUW+FFiCxLEWlNTgLPMbC98ybFsHrUGwJ8jm/edcw+b2e74vg/zzWwCfjqCNvj52A7Ej5Q6N2L/E4ETzewVfFK2DpjinJsS7LPE/OSPAyO2L3u8781sPv6kWUKUjt/ASfiE5r9m9hf8iNEifGvrLsDO+H6NP9XgOaTbRPw8b5uxYSvhRComnqy6DnxSvys+viPNbBK+0/Nm+L5r+wF/B+YkiOEC/PH+d3CCKJtH7Rj8yWMg/hd2tTk/V95HwAFm9iT+s1gCjA9axWN5BT/q8lIz645vNdoK/5q9RuqJX7TYXPDD623gBTOLnEetH36euwE1fZw4j7/OzAbj5816LRhkMhN/Et0S/12yDf7zGPPSc865+WZ2Lb6f42dm9gwV86i1wc9ht0us+6dgMv79MMrMdiYYHOCc+0cS930BPzF5X/zxy1rVPMdMxP8oe9HMXse3on/vnHs8Xe+DKl4CVpjZh/gfPRbEtQd+gEPVqw/0xv94fSHJ/YdTpoed1td/RJkDqMr6dvhhyCupPFdWd3wz7/f4hGkZvlPy/UDfKvsoiPYY+JPCh/jpM4rw81wdSOzh5BvjO73/QDAknspTIjiiTFOBHyxwM34k35rgsd4GDo2ybe9gPyNjvB7fEWeKkCrbdiqLET+i6GX8l+8q/Am8f5z7/gk/MeRP+KlSluJHIP0D6FZl283wv/p/xJ+kN4gfuChYvpyIucGCdWVzzX0UJ54WwFX4L6EV+C/CBfiTwjnAxjV8DlGPXbBuTLC+Uwrv6+5l721gjyrrhgTL1xFl2pRgG8NPajsxeG+vxSdr7wWvQ+R8WuXHOcp+uuHLi0X4z9AHwBHAX4P7DEr2/UWM6RvwrXav4FsVSony2Ymxvy3xLc6FwfH8Av+DrFG04xHr8ZN4DXbHJ2V/BP/ewSf2MfeX6vuAOJ9b/OdjNP77aVXw/v0G/4PwFCrP4RXvOZ6KT2hX4xOFJ/DzcM0GitLxPRLEMzM4HnG/m6M8xzXAM1HWDYv2nqjOey3O4xfEijXBcYv3eqdyjmmIT6S/xX+uo71/0/I+CNafi0/Wvg32tSx4b/yN6FMxPUWMedey6Z8FT0ZEJOcFrV8n4RPWVK5yISFiZi3xP5BmOudqdFmsNMRyP366n07OueqOqJU0C7qFfAc85Zw7K8Ph1Ij6qIlITjGzBma2wchGM+uHL33PUZKWHcxs06qDS8ysEX4uraZU7meaKdfiW37/nulApJKr8JWOazIdSE2pj5qI5JomwCIzm4wfTboeP5jjEPwJ9YIMxiapOQa4wczewU8GWzayrwu+VHl35kLznHM/mtkp+BHXDVz0iZalDgUDDpYApzrnYs3nmTVU+hSRnBJMFXAHvoP3FvhRpr/gB5eMds7NiH1vCRMz64lvEdmTivmwFuD7H97sol95RCSnKFETERERCSn1URMREREJKSVqIiIiIiGlRE1EREQkpJSoiYiIiISUEjURERGRkFKiJiIiIhJSStREREREQkqJmoiIiEhIKVETERERCSklaiIiIiIhpURNREREJKSUqImIiIiElBI1ERERkZBSoiYiIiISUkrUREREREJKiZqIiIhISClRywFm1tXMRppZ10zHIiIiIuljzrlMx5B2bdu2dZ06dcp0GGm1cuVKNt544w2WO+eYO3cuxcXFNGvWjC5dumBmGYhQIsU6XhJOOl7ZRccru+h4JTZt2rRfnHObRlvXqK6DqQudOnXi008/zXQYaVVQUEDv3r03WP7QQw8xcuRIPv/8cw444AAuuugihg0bVufxSWWxjpeEk45XdtHxyi46XomZ2fex1qn0mcV++eUXrrzySh555BG22WYbHnnkES6//HKWLVuW6dBEREQkDXKyRa2+aNu2LT///HP534ceeig//vhjBiMSERGRdFKLmoiIiEhIKVETERERCSklallsypQpHHLIIbRs2ZKmTZvSo0cPXnrppUyHJSIiImmiRC1Lvfnmm/Tt25fFixdz3XXX8a9//Ys//viD448/nm+//TbT4YVKaWkpt99+O926daNp06ZsueWWXHbZZaxcubLW9vHkk09y3HHHsc0222BmxJou5uuvv+baa69l7733ZtNNN6VFixb06NGDf/7znzH3vWLFCm666Sa6d+9OixYtaNu2Lfvuuy9jxoyh6nQ7o0aNSioOEREJJw0myEKlpaWcd955bLfddnz66afk5eUBflqSgQMH8sEHH7DNNttkOMrwGD58OHfddRdHH300l112GV9++SV33XUXM2bM4J133qFBg8S/V1Ldx0MPPUSbNm3YbbfdKCoqirnfhx9+mHvvvZejjjqKk08+mcaNGzN58mSuvvpqnn32WT788MPy4wv+2B922GFMnTqVoUOHctFFF7Fq1SqefvppTj/9dL788ktuvvnm8u2vuuqqpOIQEZGQcs7l3L/dd9/d5ZrJkyeX/3/KlCkOcPfee2+lbV5//XUHuBdeeKGOowuv2bNnOzNzgwcPrrT8rrvucoB78skna2Ufkct22mknt/XWW0fd9yeffOKKioo2WP73v//dAe7uu++utHzq1KkOcJdcckml5WvWrHGdO3d2rVq1qrR8/vz5ScVR30V+viT8dLyyi45XYsCnLkZOo9JnFpo6dSoA/fr1q7R80qRJAOy22251HlNYPf300zjnuOSSSyotP/vss2nWrBlPPPFEreyjQ4cOScXXq1cvWrVqtcHyE044AYDZs2dXWv77779H3X+TJk1o27btBrN/q2VVRCS7qfSZhWbMmEHz5s3p0qVL+bIffviBhx9+mD322CMn+iGVlpamNHFvmzZtopYwP/nkExo0aMCee+5ZaXnZ4ItPPvkk4b7TsY9U/fDDDwC0a9eu0vI999yT/Px8/vWvf9GpUyf22msvVq1axaOPPsq0adP4z3/+k/ZYRMJs3IxCbpkwl8VFxXTIz2NE/64M6tkx02GJpI0StSw0c+ZMevTogZnx9ddf89FHH3Hdddfxxx9/cPvtt2c6vLRYuHAhnTt3Tnr7BQsWRE1QFy9eTNu2bdloo402WNexY0emTp3K2rVradKkScx9p2MfqSgpKeHGG2+kUaNGnHTSSZXWtW7dmvHjx3PWWWdx/PHHly9v0aIFL7zwAoMGDUpLDCLZYNyMQq58cRbF60oAKCwq5soXZwEoWZOcoUQty6xcuZJvvvmG/v378/3339O1a9fydRdffDH77LNPrTzunnvuybXXXsuf/vSnpLbfe++9ueCCCzj11FOr9Xjt27fn7bffTmn7aFatWhU1wQLfIla2TbwkKx37SMUll1zCBx98wE033VTp+JZp3rw5O++8M0cddRT77rsvy5Yt49577+Wkk07i5Zdf5pBDDklLHCJhd8uEueVJWpnidSXcMmGuEjXJGUrUssznn39OaWkpu+++Oy1btuSVV15h4cKFvPzyy9x555389ttvPProo2l/3I8//jjpbUtLS5k1axbdu3ePuc3ee+/NhRdeyCmnnBJ1fdOmTTn44INTjrOqZs2a8dNPP0Vdt3r16vJtansfybrmmmu45557OOecc7jyyis3WD9r1iz23Xdfbr/9ds4999zy5UOGDGHnnXfm7LPPZv78+TRs2DAt8YiE2eKi4pSWi2QjJWpZZubMmYAfMNC6devyFq7zzz+fQw45hCeeeIKHHnqIxo0bZyzGr7/+mjVr1rDDDjtEXV+WyPXo0SPmPkpKSipdxzSRTTfdNGpy0qFDB+bMmcOaNWs2aBUrLCykbdu2CVvC0rGPZIwcOZJ//OMfnH766TH7mt1+++2sXr2a4447rtLyZs2accQRR3DPPffw3Xffse2229Y4HpGw65CfR2GUpKxDfl6UrUWyk0Z9ZpmZM2eSl5cXNQkqLS1ls802K0/Sttxyy/IRorNnz6ZBgwblCUBRURH5+fksWLAAgJdeeok99tiD/Px8dtppp/IRpACvvPLKBq1j999/P1tssQWtWrVi+PDhDBw4kDvuuKM8xu2224677rqLzp0706JFi/LWoaVLl9K8eXOKi4vZe++9ad68OVOmTNnguSxatIjNN9886X+LFi2K+nrtsccelJaWbtAiuHr1ambOnEmvXr0Svubp2EciI0eO5Prrr2fo0KE89NBDmFnU7QoLCwGfyFa1fv36SrciuW5E/67kNa78Ay2vcUNG9N+wy4BItspoomZmD5vZT2Y2O8b61mb2kpl9bmYfm9nOdR1j2MyYMYPS0tLyBKvM1KlTKSgoqNQnrHXr1qxYsQKAW2+9lW7duvHbb78BfkLWQw89lM6dO3P//fczfPhw7r33XpYtW8bVV1/NCSecwJo1awCYPn16pSk/7rzzTu644w7eeOMNfv31V9asWcMbb7xR3kI2c+ZMvv/+e9q2bcvcuXOZPHkyo0eP5pdffqF9+/Y8/PDD7LbbbqxYsYIVK1Zw4IEHbvA8y/qoJfsvVh+1E044ATMrTyLLPPjgg6xatYqTTz650vJ169bx1VdfsXDhwmrvI1U33HAD119/PaeeeioPP/xw3Al4d9xxRwDGjBlTaXlRUREvv/wyrVu3ZrvttqtRPCLZYlDPjowa3J2O+XkY0DE/j1GDu6t/muSUTJc+xwD3AI/FWH8VMNM5d7SZdQPuBfrF2DbnlZSUMHv2bNasWcMBBxzABRdcQLt27Zg1axYPP/wwu+66K9dcc0359q1bt+aPP/6gsLCQgoIChg8fTmFhISUlJdxzzz0888wzLF++nBEjRjB+/Pjy6SeGDBnChRdeyPz589lxxx2ZPn06ffr0Afw8Xtdeey2vv/56eSvbsGHDuO+++yolameddRann346wAatf1UTv2jS1Uete/fuXHDBBdxzzz0MHjyYww8/vPyqAgcddNAGoyoLCwvZYYcdOOiggygoKKjWPgDeeust3nvvPQB+/vln1q5dyz/+8Q8Att566/KE+t577+W6665jq6224uCDD+app56qtJ927dpVGhxwySWX8Nhjj3HFFVcwa9Ys9ttvP5YtW8aDDz7IkiVLuPfeeyuVgB9//HG+//77hHGIZKtBPTsqMZPcFmsm3Lr6B3QCZsdY9xpwQMTf84F2ifaZq1cmmD17tgPckCFDXI8ePVyTJk1cfn6+69Gjhxs1apRbvnx5pfsMHDjQPfLII+6vf/2rGz16tHvqqafcWWed5Z5//nm3//77O+ece+mll5yZuVatWlX617RpU/ftt98655zbcsstXUFBgXPOuXHjxrltt9220uO8/vrrlWa8b9eunXv77bfL/37//fddu3btyv/u16+fu++++9L6+sSzfv16d+utt7ouXbq4Jk2auA4dOrjhw4e7P/74Y4NtFyxY4AB30EEHVXsfzjm36667OiDqv8h9Dx06NOZ20eJwzrl58+a50047zXXs2NE1atTItWjRwh1wwAFRr0hx0EEHpbTv+kozp2cXHa/souOVGHGuTGCuykWc65qZdQJedc5tUNY0s5uAPOfccDPbE5gK7OWcmxZl23OAcwDatWu3+9ixY2s38Dq2YsWK8ikb7r77bnbeOXEV+Oabb6ZDhw6MHz+eRx55hNmzZ/PGG2/w22+/cfzxx7P//vvz2muv8dZbb3HnnXdG3cfy5cs5+uijGT9+PM2bN+e1115j/Pjx3H///eXb3HrrrRQVFfGPf/yDZcuWccwxx/Dyyy/TsmVLwPd/+/DDD8uvQTlw4EBGjx4dc7BBLlixYgXNmzfPdBiSJB2v7KLjlV10vBLr06fPNOdc9A7PsTK4uvpH/Ba1lsAjwEzgceAToEeifeZqi9pf//pXZ2bu999/T+o+w4cPd5tvvrkbPny4c8659957z7Vv395tv/32rqSkxDnn3MyZM93GG2/s3nnnHVdaWuqKi4vd1KlTy68ROWHCBLfNNtuU7/ODDz5wTZo0cR999JFbvXq1+89//uMaN27srrvuOuecc2+88Ybr1KlTpThOP/10d9VVVznnnCspKXENGzZ0H330UY1ej7DTL8jsouOVXXS8souOV2Jk67U+nXO/O+dOd871AE4DNgW+zWxUmTNjxgy23nprWrRokdT2rVu35ueffy6/RmWrVq1YunQpw4cPL++wvuuuu3L33Xdz/vnn06JFC7bccktGjhxZPt3EjBkz6NmzZ/k+9957by699FL69+9P586d+fXXX9l2223L+7fNnDlzg/5nkfto0KABf/3rXzn00ENp3rw58+bNq9FrIiIiksvCXvrMB1Y559aa2dn4/mqnJdpnr1693Keffpr2WDOpoKCA4447jn322Yfx48dnOpxyX3zxBXvuuSe//vpr+Sz94o9X7969Mx2GJEnHK7voeGUXHa/EzCxm6TOjoz7N7GmgN9DWzH4ArgMaAzjn/gPsADxqZg74AjgzQ6GGQioTwNaWjz76iDZt2rDddtvx+eefc8opp3DNNdcoSRMREakFGU3UnHNDEqz/AOhSR+FIEmbNmsVVV13FqlWr2GqrrTjvvPO44IILMh2WiIhITsr0PGqSZc466yzOOuusTIchIiJSL4R6MIGIiIhIfaYWNRERqWTcjEJumTCXxUXFdMjPY0T/rpr9XyRDlKiJiEi5cTMKufLFWRSvKwGgsKiYK1+cBaBkTSQDVPoUEZFyt0yYW56klSleV8ItE+ZmKCKR+k2JmoiIlFtcVJzSchGpXUrURESkXIf8vJSWi0jtUqImIiLlRvTvSl7jhpWW5TVuyIj+XTMUkUj9psEEIiJSrmzAgEZ9itSRBFcdUqImIiKVDOrZUYmZSG1atw5efx3GjIFXX427qUqfIiIiInXhs89g+HDo2BEGDYIPPoCLL457F7WoiYhI6GjSXckZP/8MTz3lW89mzoTGjeGoo2DYMOjf3/99220x765ETUREQkWT7krWq1raXL8edt8d7r4bhgyBTTZJeldK1EREJFTiTbqrRE1C7fPPfXL2xBO+JW2zzXxpc+hQ6N69WrtUoiYiIqGiSXclq/zyS0Vpc8YMX8o88kg4/fSK0mYNKFETEZFQ6ZCfR2GUpEyT7kporFsHb74JjzziS5vr1sFuu8Fdd/nSZtu2aXsoJWoiIhIqI/p3rdRHDTTproTErFkVpc2ffvKlzYsu8qXNXXaplYdUoiYiIqGiSXclVH75BZ5+2ido06dDo0a+tDlsGBx2WI1Km2Wjm5u03273WNsoURMRkdDRpLuSUWWlzTFj4JVX/N89e8Kdd/rS5qab1vghqo5ujkWJmoiIpJXmQJOsVbW0uemmcOGFvrS5665pfahoo5ujUaImIiJpoznQJOv8+mtFaXPatLSWNuNJdhSzLiElIiJpE28ONJHQWL/ej9Y89ljYfHM/IKCkBO64AxYvhhdf9FcPqKUkDZIfxawWNRERSRvNgSahNns2PPooPP44/Pijn0bjggt8abNHjzoNJdro5miUqImISNpoDjQJnWXLKkqbn37qS5tHHOEnpD3sMGjSJCNhRY5uXhJnOyVqIiJpVN870tflHGj1/bWWONavhwkTfHI2fjysXevnObv9djjpJD//WQiUjW62K+dNi7WNEjURkTRRR/q6mwNNr7VE9cUXFaM2ly71pc3zzvMDA+q4tJkuStRERNJEFxP30j0H2gYtZ7uWcMuHeq0lsGwZjB3rE7RPPqkobQ4bBocfnrHSZrooURMRSRN1pE+/aC1nhb+VUFjUMOr2eq3rifXr4a23fHL28su+tNm9O/zf/8HJJ4emtJkOStRERNJEHenTL1orZalzNDSjxLkNttdrnePmzPHJ2eOP+9LmJpvAuedWlDbNMhxg+ilRExFJE11MPP1itZCVOEde44Z6reuD336rKG1+/DE0bFhR2jziiKwvbSaiRE1EJE10MfH0i9VK2TF4bfVa56j16+Htt31yNm6cL23uvDPcdpsvbbZrV+chZWqUsRI1EZE00sXE0ytaK2UDs/KTpF7rHPPllxWlzSVLoE0b+POffetZz54ZK21mcpSxEjUREQmtaK2UHVuXKEHLJdFKm4cfXlHa3GijTEeY0RHdStRERCTUqracFRQUZC4YSY+SksqlzTVrMl7ajCeTI7qVqImIiEjd+OqritLm4sW+tHn22b71bLfdQjtqM5MjuhvU+iOIiIhIvdVoxQq4/37YZx/YYQe49VaflD3/vE/W7r4bdt89tEka+L6SeY0rz91XV6OM1aImIiI5S9cDzZCSEnjnHRgzhn1feAHWrYOddvJJ2sknQ/v2mY4wJZkc0a1ETUREcpKuB5oBX30Fjz4Kjz3mW8tat2bJEUfQ8e9/D32rWSKZGmWs0qeIiOSkeCP1JI2KiiqXNv/1Lz+VxnPPwZIlfHPxxdCrV1YnaZmkFjUREclqscqbuvZqLSopgYkT/cCAl16C1athxx3hllt8aXPzzTMdYc5QoiYiIlkrXnlT115NLOU+fHPnVpQ2CwuhdWs44ww4/fSsL22GlRI1EZE6pM7t6RWvvKlrr8aXdB++5cvhmWd869kHH0CDBjBgANx+Oxx5JDRtmoHo6w8laiIidUSd29MvXnlT116NL+5s+7u0h0mTfHL24ou+tFnW/+yUU3KitJktP5qUqImI1JFMXoYmlxQVr2O/0ZNYXFRMAzNKnNtgm7Lypq4HGlu0JLfzskKOnTIR7h0KP/wA+fm+rHn66Tk1ICCbfjQpURMRqSPq3F5z42YUUvhbMYVFfvLRaEmaypvJKevD12LNSo748n8cO3sivQq/pMQawID+/nJORx2Vk6XNbPrRpOk5stDvv/9OgwYNMDP23HPPqNssX76czTbbDDOjVatWuIgvs99++41rrrmGHj160Lx5c/Ly8thqq60YMGAADzzwQNTHifXvvffeq/XnW1payu233063bt1o2rQpW265JZdddhkrV66stX1U9zFXrVrFNttsQ58+fbjwwguT2tbMom47d+5cTj75ZHbYYQdatWpFs2bN6NatG5deeilLlixJ+rlLeMTqxK7O7cm7ZcJcSqMkZw3NMKBjfh6jBncP3ck2dEpK+Ff+T9z96q18cs+pjJ5wD/nFf3Br3zN4+82P4fXX4fjjczJJg+z60aQWtSw0ffp0nHPk5eUxZ84cnHNYlebokSNH8vvvvwPQs2fP8vVffPEFhxxyCEVFRZx00kmceeaZmBnffPMNL7/8MhMmTOCcc86p9DgnnXQShx12WNRY9thjj1p8pt7w4cO56667OProo7nsssv48ssvueuuu5gxYwbvvPMODRok/r2R6j6q+5jXXnstP//8c1LPK9G2P/zwA0uWLOHoo49miy22oFGjRsyaNYsHHniAsWPHMnPmTDbbbLOkHkvCQZ3ba25xUTFsueHyUudYMPqIug8o23zzTfmozf0WLWJti1a8uvsAHutyED/vsCsjBnRjQD1IcrNqRLBzLuf+7b777i7XTJ48ufz/t912mwPcSSed5AA3b968Stt++eWXrnHjxu6EE05wgLv00kudc86Vlpa6HXfc0bVs2dLNmjVrg8coKSlxixcv3uBxJkyYUDtPKgmzZ892ZuYGDx5cafldd93lAPfkk0+mfR/Vfcxp06a5hg0blr9uF1xwQcyYUtm2qmeffdYB7uabb076PhJf5Oertr00/Qe376iJrtPlr7p9R010L03/oc4eOxfsO2qiu+uJcW7ry1+t9G/fURMzHVp4LV/u3IMPOrfffs6Bcw0aODdggHNjxzpXXFzrD1+Xn69kvTT9B9ft6jcqvYe6Xf1Gxj6PwKcuRk6j0mcWmjZtGgBnnHEGALNmzaq0fvjw4WyxxRb06dMHgN122w2Azz//nDlz5nDooYey8847b7DfBg0asHnESJ5p06ZhZnXSahbL008/jXOOSy65pNLys88+m2bNmvHEE0+kfR/VecySkhLOPvtsBgwYwODBg+PGk8q20Wy99daAL2FL9hnUsyPvX9GXBaOP4P0r+qpEl6IR/bvSoEoFQa2SUZSW+mttnnKKv67m2WfDr7/C6NGwcCG88QaccELOljYTGdSzI6MGd6djfl7oS+YqfWah6dOns9VWW3HAAQfQuHFjZs+ezaBBgwB49dVXefPNN3nhhRcoKCgAYPfddwdgxYoVAMyfP59Vq1bRrFmzhI+z9dZbU1JSwi+//LLB+rZt20a9X2lpKcuWLUv6+bRp0yZmKfGTTz6hQYMGG/TFa9q0KT169OCTTz5JuP9U91Gdx7z99tv56quveOGFFxLGk8q2AKtXr2bFihWsXr2aOXPmcPnllwNw+OGHJ3V/kVwyqGdHxi2dQ8f8hqGfViEj5s3zpc1HH4VFi6BVKxg6FIYNgz33zJlRm+mQLSOClahlmRUrVvD1119z1FFH0aRJE3bcccfyFrV169Zx2WWX0bdvXwYPHsz//d//0bx5c7p06QL4lrVtttmGGTNm0LFjRwYMGEDfvn0ZMGAAW265ZdTHKS0tZdNNN90gjs0335zFixdHjXHhwoV07tw56ee0YMECOnXqFHXd4sWLadu2LRtttNEG6zp27MjUqVNZu3YtTZo0ibn/VPeR6vYLFizguuuu49prr6VTp0589913cZ9rstuWeeihh7jooovK/+7UqRNPPPEEBxxwQML7Sm7Jlnmfalt+XmPev6J3psMIj99/99fVHDMG3nvPJ2OHHurnPBs4EPJC2O9KkqZELcvMnDmT0tLS8nJmjx49+PjjjwG48847mT9/Ps8//zylpaV89tln9OjRo7y1Ki8vjw8++IDbbruN5557jrFjxzJ27FjMjAEDBvDggw/SsWPHSo9z8cUX86c//WmDOFq3bh0zxvbt2/P2228n/Zzat28fc92qVauiJkzgW7jKtomXqKW6j1S3P/fcc9lmm2249NJLY8ZQJpVtywwaNIhu3bqxYsUKZsyYwfjx46O2cEpuy6Z5n6QOlJbC5Mk+OXvhBSguhq5dYdQoX+7cYotMRyhpokQty5T1T4tM1J588kkWLlzIjTfeyJ///Ge6d+/OV199xYoVK8q3K7PZZptx8803c/PNN7NgwQLefPNN7r77bt544w3OPvtsXn/99UqPc9RRR9G3b9+UYmzatCkHH3xwTZ8qAM2aNeOnn36Kum716tXl26RzH6ls/8QTT/D2228zZcoUGjduHDeOVLaNtMUWW7BF8KU7aNAgjjnmGPbYYw9WrVrFlVdemfR+JLtl07xPUovmz68obS5c6Eubp53mS5t77aXSZg5SopZlpk+fDlRO1NavX88JJ5xAo0aNuPHGGyttV9Y/LZrOnTtz3nnnccwxx9CuXTumTJmywePstNNOKcdYUlKS9BQVAJtuuikNGzaMuq5Dhw7MmTOHNWvWbNDKVVhYSNu2beO2plVnH8luv2bNGi699FIOP/xw2rdvz7x588q3AT+X3bx582jbti15eXlJb5ufnx/3+eyyyy707NmTf//730rU6pFsmvdJ0uyPPypKm//7n0/GDjnEDwwYNEilzRyX0VGfZvawmf1kZrNjrG9lZq+Y2Wdm9oWZnV7XMYbNtGnTaN++ffnozB49egDw4Ycfcv3119OmTZvy7YANWtSiadKkCQ0aNKBVq1aVHqdt27a0a9cu5RgXLVrE5ptvnvS/RYsWxdzXHnvsQWlpaXl5t8zq1auZOXMmvXr1ShhPqvtIdvvi4mJ+/vlnXnvtNbbffvvyf7179wZ8C9r222/PQw89lNK2ySguLk5pwEZdGzejkP1GT6LzFa+x3+hJjJtRmOmQsl66J8vVMQq50lJ/rc3TTvOjNs88E378EW66ybekTZgAQ4YoSasHMt2iNga4B3gsxvoLgDnOuSPNbFNgrpk96ZxbW1cBhsmqVav46quv6N+/f/my/Px8br75ZtavX895551Xvnz69Onk5eWxww47APDee++xyy670LJlyw32e8MNN1BaWsrJJ59c6XH233//asWZzj5qJ5xwAjfddBN33HFHpc7zDz74IKtWrSqPGfxgivnz59OsWTO22mqrau0jle033nhjnnvuuQ1i/vnnnzn//PMZMGAAZ555JrvssktK25ZZunRp1Ndm8uTJzJ49uzzJCxv1paod6ZwsNxuPUeRAiit6lFI0ozC0sdbI/Pnw2GO+tPn999Cype9zNmwY7L23Spv1UEYTNefcFDPrFG8ToIX5afWbA8uA9XURWxh99tlnlJSUbNBK9re//W2DbWfOnMmuu+5aXlK89tpr+fTTTxk4cCB77LEHzZs3p7CwkBdeeIHPPvuM/v37c/3111d6HCDmPGVHHHFEzAEF6eyj1r17dy644ALuueceBg8ezOGHH15+lYCDDjqIk046qXzbwsJCdthhBw466KDyqUlS3Ucq2zdu3Jhjjz12g5jLRnJuu+22ldansi3Aeeedx5IlS+jbty9bb701q1evZtq0aYwdO5YWLVpw2223pfRa1hX1paodZa9dvFGfyY4KzbZjVDWxXFtSGvrEMiV//AHPP+9Lm1OmVJQ2R41SaVMwF+WaaXUagE/UXnXObTADq5m1AMYD3YAWwAnOuddi7Occ4ByAdu3a7T527NhaizkTVqxYwdtvv81dd93FDTfcEHdqhsLCQk455RQGDhxYPmnr1KlTee+995gzZw6//fYbK1eupGXLlmy//fYceuih9O3bt/wyUy+99BJ33XVXzP2bGePHj6d58+ZpfY6xlJSU8MILL/Dqq6+ydOlSWrVqRe/evTnjjDPIi/gCW7p0KUOGDGHXXXfljjvuqNY+qrt9pLI4Bg0axMUXX1ztbSdPnsxbb73F/PnzKSoqwsxo164dvXr14oQTTqhWWbouzCpcHnNd946tYq7LpBUrVtTZ+7m2FBWvo/C34krXwWxgRsfWeeTnVR68km3HaO7SP1hbUlr+d7s8+LEYmjRsQNf2LTIYWQ2UlpL/2We0f/NNNp0yhYarV7Nqiy1Y2r8/Px56KGty6PJwufD5qm19+vSZ5pyL2pcn7InascB+wKXAtsDbwK7Oud/j7bNXr17u008/rYVoM6egoCC0pS7ZUH0+XvuNnhT1Gnod8/N4/4rURhDXlWjHK9vmLEvldc+2Y9T5iteIPFNd1n09t81qhEH2Xd/z228rRm1+/z20aAEnnuhLm/vsk5Olzfr8fZgsM4uZqIX9ElKnAy8Gl8KaByzAt66JSEiN6N+VvMaVR/Fm2yV+ykpthUXFOCr6cIW5w30qo0Kz7RileyBFnVuxwpc1e/eGbbeFG2+ELl3gySdh6VJ44AHYd9+cTNKyWVgG3KTUR83MdgQOBLYC2gLFwE/ATGCKc+6PNMe3EOgH/M/M2gFdgW/T/BgikkbJ9KUKu2zrwwU+aYnWShYtmcm2Y5TOgRR1prTU9zcbM8b3P1u5ErbbDv7xDzj1VIgY8CThE6YBNwkTNTPbAt/36wyg7IrdVdN+B5SY2TvAffhSZsKaqpk9DfQG2prZD8B1QGMA59x/gBuBMWY2K3jMy51zmpJdJOSy5Rp6sWTjnGWpJjPZdIyqJpZNGjYI7QW0WbAAHn2UlQ8+zMaLF/FHkzwm79qH/PPO5sBhA9VqliXC9GMtZqJmZm2AkcCf8cnTd8BTwCfAUvwIzDxgE3w5ch980tUfP43GZc65N+I9uHNuSIL1i4FDk3omIiIpiDfdQyqtU2GRba1kqYpMLAsKCugdpue1YoW/jNOYMVBQgDPjs049eOZPxzOhyz6sbtyUvG8bMmrm4pw5HrkuTD/W4rWozQM2Ah4CHnXOfRxnWwDMrCVwIr4F7lUzG+6ciz18UEQkAxJN9xDWUluiAQ7Z1EqW9UpL/VUCxozxVw1YubK8/9kxK7ZlOpXnrAx76VwqC9OPtXiDCR4HtnHOXZBMkgbgnPvdOfdAMHLhGHz/NRGRUIlX1gCf8Iwa3J2O+XkYfjRkpktt2TjAISctWADXX+/7m/Xu7fufnXiiT9q++QauvpoZbDixOIS7dC6VhWnATcwWNedc/AmgEnDOjavJ/UVEaksyZY2wtU6Fqc9MvbNyZUVpc/Jkv6xfP7jhBjj6aNh440qbh6k1RqonTF0JMn0JKRGROpeNJ9Iw9ZmpF5yrXNpcsQK22cYnZ6edBltvHfOuYS2dS2rC8mMt6UTNzBoCGznnVlVZ3hcYCKwCHnDOLUhviCIi6ZWNJ9JsTC6z0nffVVxr89tvoXlzOP54PyHt/vsnNWozTK0xkv1SaVG7FTjPzNo555YDmNmJwJNUTNdxlpnt5pxblOY4RURqpGpH/GN278jkr34O/3QPgWxMLrPGypXw4ovwyCMVpc2+fWHkSBg8eIPSZjLC0hoj2S+VRO1AYHJZkha4DigCLgbaA6Pwl3sanq4ARURqKtrklS9MKyxPzkI33UMUaqVJM+fgvfd8afPZZyuXNk89FTp1ynSEIkBqidqWwNSyP8xsG/yVAm5wzj0RLDsQGIASNREJkVzpiK9WmjT4/vuK0ub8+b61rKy0ecABjJu5mFvGzmVx0RdKhiUUUknUWgKRF0PfD39Fgjcjln0B9ElDXCIiaaOO+PVcWWlzzBiYNMkv69MHrr3WlzabNwfCddkgkTKpJGpLgM4Rfx+Mv9bntIhlzYH1aYhLRCRt1BG/HnIO3n+/orT5xx/QubOfA+2006KWNnOl5RUST44s2SOVRO1D4Cgz+xOwGjgWmOicWxexTWdAsy+KSKioI349snChL22OGVNR2jzuuPLSJg1iz/OeKy2vahnMLakkajfhp+F4Ofi7FPhn2UozawocADyftuhERNJAHfFz3KpVlUubzvmrBlxzDRxzTHlpM5FcaXnNpZZBSSFRc87NMrO9gKHBomecc59EbNITmAQ8ncb4RETSQh3xc4xzMHWqT86eecaXNjt1guuu86XNzp0T7WEDudLymistg+KlMuHtVsDPzrm/RlvvnPsAODpdgYmIiGxg4UK2euIJOPtsmDcvpdJmIrnS8porLYPipVL6XACMAc6snVBERESiWLUKXnrJt55NnMg2zsFBB8Hf/w7HHpt0aTMZudDymistg+KlkqgVAb/WUhwiIiIVnIMPPqgobf7+u7++5rXX8mGXLux90kmZjjC0cqVlULxUR332rK1AREREWLQIHn/cJ2jffAPNmvlWs9NPhwMPhAYNWF1QkOkoQy8XWgbFSyVRGwn8z8zOcs49VEvxiIjkvEzOcRXK+bVWrYJx43xy9s47vjXtwAPhyit9ktaiRWbjE8mgVBK1w4AC4H4zOw/4GFiKvzpBJOecuzE94YmI5JZMznEVqvm1nIMPP/QXQo8sbV5zjR+1ue22dRuPSEil2qJWpiexy6AOUKImIhJFJue4CsX8Wj/8UFHa/Pprihs35fWu+1Kw9+H0O/d4Bu2+Zd3EIZIlUknUdA1PEZEayuQcVxl77OLiitLm22+Dc/zScy9u/9MljNtuX1Zu1AyAd8Z9AQ0aZL4UKxIiqUx4+25tBiKSi0LZH0gyKpNzXNXpYzsHH31UUdpcvhy22gquvhpOO42Bzy/cIJZsnD1fn3GpbdWfGVBE4irrD1RYVIyjoj/QuBm6HG59NqJ/V/IaN6y0rK7muKqTxy4shNGjYYcdYJ99fJnzqKNg4kRYsABuuAG22y4nZs/XZ1zqQsqJmpntYmajzexlM3snYnknMzvezFqnN0SR7BSvP5DUX4N6dmTU4O50zM/DgI75eYwa3L1OWmFq7bGLi2HsWBgwwLeaXXklbLopPPQQLF3qL5Let2+lqwbEasXLptnz9RmXupBKHzXM7AbgKioSvMgRnw3w1/m8BLg7HcGJZLNcaDGQ2pHJOa7S9thlpc0xY3yStnw5bLklXHUVDB0K220X9+65MHu+PuNSF1K51ueJwNXABOBy4ATgirL1zrlvzexT4CiUqInoenuSmwoL4YknfIL21VeQlwfHHOOvtdmnT9LX2syF2fP1GZe6kEqL2l+AecBA59xaM4t2AfYvgd7pCEwk2+VCi4EIAKtXw8sv++TsrbegtBT239+XNo87Dlq2rNZus332fH3GpS6kkqh1B8Y459bG2WYx0K5mIYnkhlxoMcgWGnlXC5yDjz+uKG0WFcEWW/j+Z0OHwvbbA2Wv/af18rXXZ1zqQiqJmgGlCbZpB6yufjgiuSXbWwyyQahm288FixdXlDa//BKaNq1c2mxYMWpUr70+41L7Uhn1+Q2wb6yVZtYA2B/4oqZBiYgkSyPv0mD1anj2WTj8cD8g4PLLoXVreOABP2rziSfg4IMrJWmg116kLqTSovYs8A8zu8w5d1uU9VcB2wF3piUyEZEkaORdNTkHn3ziW86efrqitHnFFb602aVLwl3kwmuvsrmEXSqJ2h3AccC/zOx4gqk5zOxW4ACgF/Ah8ECaYxSpl3QCSY5G3qUoWmlz8GBf2uzbd4NWs3iy/bVX6VayQdKlT+dcMf56n48DuwF74vutXQrsDjwBDHDOra+FOEXqFc14nrx0zrY/bkYh+42eROcrXmO/0ZNy5/VevRqeew6OOCJ6afPJJ+GQQ1JK0iCzV1lIB5VuJRukNOGtc245MMzMLgX2ADYBlgMfO+d+roX4ROqleCcQ/dKvLF0j73KudcU5+PTTitLmb79Bx44+SRs6FLrWPJnK9lGPuVC6ldyXUqJWxjm3DD/xrYjUAp1AUpOOkXc5kxwvWVJR2pwzx5c2jz7alzb79Uu51SyRbB71mO2lW6kfki59mlmJmV2TYJu/m5lKnyI1lAvXQcw2WZ0cr1kDzz8Pf/qTL23+7W/QqhXcf79P3J56Cg49NO1JWl2KLEvPXfpHWsrS2V66lfoh1XnULMntRKQGNON53cu61hXnYNo033L21FO+tNmhA4wY4Uub3bplOsK0qVqWXltSmpaydLaXbqV+qFbpM47WaMJbkRrTCaTuZU1yXDav2Zgx8MUXsNFGFaXNKHOd5YLaLEtnc+lW6oe4iZqZHVhlUacoywAaAlsBJwMaLiOSBjqB1K1QJ8dr1sArr/jk7M03oaQE9t4b/vMfOOEEyM/PdIS1KqvL0iI1lKhFrYBgvrTgdmjwL5qyS0xdlpbIREJAc5nVjrC+rqFKjp2D6dMrSpvLluVsaTORrCtLi6RRokTtBnyCZsC1+MTt3SjblQC/ApOdc1+lM0CRTMm56RpCQq9rAmXzmo0ZA7Nn+9LmoEG+tFmNuc5yQdaUpUVqQdxEzTk3suz/ZjYUGOecu6u2gxIJg5yZriFkrn/li5x8XWvUSrh2Lbz6qk/OXn/dlzb32gvuu8+XNlu3rtXYw65qWbpJwwaMGtw97usb1lZbkVQlPZjAOde5NgMRCRv1i0m/cTMK+W3Vuqjrsvl1HTejkBHPfca6Ut9TpLComBHPfQbEaSV0DmbMqCht/vorbL45XHaZbz3bYYe6CT5LRJalCwoK6J0gSVOrreSKlEd9mlljoB+wA9DcOXdjsLwp0BL4xTlXmtYoRTJA/WLSL96leVrlNa7DSNJr5PgvypO0MutKHSPHf7FhYvDjjxWlzVmzoEmTyqXNRsl9LavFKDa1hksuSSlRM7MBwH+B9vh+aw64MVjdA3gfOAV4On0himSG+sWkX7xWMwvRDIypJkFFxdFbCcuXRytt7rkn/PvfvrTZpk3K8anFKDa1hksuSTpRM7NewDjgF2A4/qLsQ8rWO+c+NLMFwNEoUZMcEOrpGkIm2cQmVislQFGMkmhdS1sS5Bw7/fQtXHyxb0H79Vdo396XNocOhR13rHaMajGKT63hkktSaVG7BlgF9HLOLTWz66Js8wmwW1oiEwmBUE3XEFKpJDYj+ndl+DMzcRvsJTwn0eokQa2bNS7ve7fJyiIGzSng2FnvsMPP3/nS5sCBcPrpKZU241GLUXxqDZdckso3xn74UZ9L42yzCDiiZiGJSDZJJbEZ1LMjn36/jCc/XFgpWQvTSbQ6SdDIAdvz5r8eZvBnb9P7209pXFrCZ5t34bMr/smuI85NubSZiFqM4lNruOSSVBK15viyZzzNSOFC7yKS/VJNbP4xqDu9tm4T2pNoSknQzJkwZgwDn3ySgb/8wi8t2vBwr4H8b98jOPa0/rX2nNRilJhawyVXpJKoFQI7JdimB/BttaMRqSdyacResolNtjznhEnQTz/56TTGjIHPPvOlzaOOgmHDaNu/P39u1Ig/13KMajESqT9SSdTeAM41s/2dc+9VXWlmhwH7AqPTFZxILsq1EXvJtO5k03OOlgT9rW9nBn7/CVx/Abz2GqxfD716wT33wIknwiabZCTOsL12IpJ+qSRqo4ATgbfM7G6gE4CZHQEcCFwALAH+L80xiuSUXBuxl0zrTrY95/Ik6LPP4JFH4Ign4ZdfoF07uOQSP2pz550zHaaI1AOpXJmg0MwOBZ4FRkSsGo+fU20+MNg5l6gfm0i9losj9hK17mTVc/7554rS5syZ0LixH7U5bBj075+WUZsiIslK6RvHOTfdzLriR3buA2wCLAc+BF52zq1Pf4giuaU+jtgL+3O29evh5Zd9cvbqq760ufvucPfdMGRIRkqbIiJQjUtIOedK8K1o42v64Gb2MPAn4Cfn3AZ1BDMbAZwc/NkIf9mqTZ1zy2r62CKZUh9H7IX2OX/+OYwZwz6PPAJFRbDZZn6C2qFDoXv3zMYmIkI1ErU0GwPcAzwWbaVz7hbgFgAzOxIYriRNsl19HLEXquf8yy8Vpc0ZM6BxY5bvvTeb/u1vvrTZOHuvOSoiuac6F2XfBdgV2AKI9o3myi7UnohzboqZdUryoYegS1NJjqiPI/Yy+pzXrYM33qgoba5bB7vtBnfdBUOG8MXs2fTu3TszsQmQPdO3iNS1VK712QZ4HBhQtijGppEXak8LM2sWPO6F6dyviOS4oLTJE0/4QQKbbQYXXeRLm7vskunoJJBN07eI1DVzLtpV96JsaPYYcArwDvAEfgLcqIMHnHPvJh2Ab1F7NVoftYhtTgBOcc4dGWebc4BzANq1a7f72LFjkw0hK6xYsYLmzZtnOgxJko5X5jRevpzNJk6k/Ztv0uKbbyht1Ihf99mHpQMGsGzPPXFRRm3qeGXW3KV/sLakdIPlTRo2oGv7Fhss1/HKLjpeifXp02eac65XtHWpJGrLgDnOuf3TGVySidpLwHPOuaeS2WevXr3cp59+mqYIw6GgoEClmSyi41XH1q2DN9/0rWevvOL/7tnTT6lx0knQtm3cu9eH4xXm0mLnK14j2pnIgAWjN7x8dH04XrlExysxM4uZqKXSR60hMDU9ISXPzFoBB+Fb80REKsyaVVHa/Okn2HRTuPBCX9rcdddMRxcaYS8thn36FpFMSiVRmw5sk84HN7Ongd5AWzP7AbiOYICCc+4/wWZHA28551am87FFJEv9+is8/bRP0KZN8xPQHnmkbz077LC4ozbD3KpUm8J+ZYjQTt8iEgKpJGo3Aq/HutZndTjnhiSxzRj8NB4iUl+tX19R2hw/3pc2e/SAO+7wpc1NN024i3itSvm1Fng4hP3KEKGavkUkZFK5hNQkMzsReMnMXsW3sC2PsW3UedFERFIye3ZFafPHH31fswsu8KXNHj1S2lW8VqV/7t0gfTGHUDaUFuvjlDUiyUhleo4mwECgNTA0+Fe1/6cFy5SoiUj1RCtt/ulPFaXNJk2qtdv4rUobVzvc6qrLMqxKi7HV13K4ZI9USp+j8MnZHOAZYDExpucQkdqRsyeV9ethwoSK0ubatX4wQAqlzUTC1KpU1537VVqMLuyDLEQgtUTtRGAWsIdzbm0txSMiMeTkSeWLL3xy9vjjFaXN887zrWcpljYTiduqtPybtD5WIpno3J/NpcUNfqDsWpL4TkkI+yALEYBUOmbk40dfKkkTyYB4J5WssmwZ/PvfsOeesPPOvtVs773hpZegsND/neYkDXyiMmpwdzrm52FAx/w8Rg3unpETctg794dJ2Q+UwqJiHP4HSuFvxYybUVjjfes4SDZIpUXtS2Dz2gpEROLL6pPK+vXw1lu+9ezll31pc5dd4PbbfWlzs83qJIywtCqFqQwbdtF+oJQ6l5ZWLx0HyQaptKjdBgwysy61FYyIxBbr5BHqk8qcOfC3v8GWW8IRR8CkSXDuuTB9OsycCZdcUmdJWpiM6N+VvMYNKy1T5/7oavMHio6DZINUWtQKgTeBj8zsTmAasafnmJKG2EQkQqZH7iU9kGHZMhg71reeffIJNGzok7Rhw/xtNUdt1lSYBmLUVuf+MD3HdKnNVi8NspBskEqiVoCfesOAa9lwao5IDeOsE5FqyORJJeFAhvXr4e23+eH2+9hs0ps0KVnHvPadKb70Orr/7Txo167WY4wnjAMx0l2GDeNzTIdoP1AamKXtB0pYyuEisaSSqN1A/ORMRNIoVutIJk4qsQYyPPfYBAaNnetHbS5ZQvO8FjzZYwDPdz+YLzbbhrwmjRi1eD2DMpun1YvRfbn6HKP9QOnYuiSrn5NIKlK5MsHIWoxDJBTCUjoKW+tIZH+glqtXcNSXUzh21jv0WPK1L20efjhXtdyd59vvytpGFdfaDEuikNUDMZKUy8+x6g+UgoKCzAUjUsdSaVETyWlhSo6q0zpSm0nmFi2bsM3MDzh21jsc+s2HbFSyjq/abs1dh5/LXx4eCe3a8fQVr0Vtcg9DolAfRvfVh+eYirD86BKpKSVqIoEwlY5SbR2ptSTzyy/h0Ud56+Ex5P38I781bcHTu/bnue4H8+0WXRh1zC7l/c/CnChkeiBGXagPzzFZYfrRJVJTMRO14MLrf3fOfZbqTs1sI+B8YLVz7r4axCdSZ8JUOko16Ulrkvnbb/DMM37U5kcfQcOG5B12GB8fdCR/W70V368ooUN+HqOqtFDUZaKQamtJfRjdVx+eY7LC9KNLpKbitah1Baab2VvAGOBl59zqeDszsx2A04J/mwJnpClOkVoXphahVJOeGieZJSXwzjvwyCMwbhysWcO37TrxdJ8z+HCfwzjz2H0Y1LMjBXF2UVeJQnVbS+rD6L768ByTEaYfXSI1FS9R2xG4GLgKOBRYa2bTgU+BJcBvQFNgE6AbsDfQET99x1vAX51zs2svdJH0ClPpKNWkp9pJ5ldfwaOPwmOPweLF0KYN3w4awojmuzFtk85gBiUkXTaKTBTKWr2GPzMzrUmbWkskkTD96BKpqZiJmnNuHXCrmf0bOBk4E9gT2KdsE3xSVuZn4EHg3865z2snXJHaE7bSUSqtIyklmUVFFaXNDz/0ozYHDIA774Qjj+TU29/f4CSXaiJUm32E1FoiiYTpR5dITSUcTOCcW4VPwB40s5b4RG0rfEtaMfAT8Llz7ovaDFSkLmRr6ShhkllSAhMn+uTspZdg9WrYcUe45RY4+WTYvOIyvulIhGqz1UutJZJI2H50idRESqM+nXO/AxNqKRYRqYGoSebcuRWlzcJCaN0azjzTX85p9919abOKdCRCtX19xrC2lmhKiPDI1h9dIlVpeg6RXLN8eUVp84MPoEEDX9q8/XY48kho2jTu3dORCNXH6zNqSggRqQ1K1ERyQbTS5g47wL/+BaecUqm0mUg6EqHabvUKY2uJBjmISG1QoiaSzb7+uqK0+cMPkJ8PZ5zhS5u9eoGZL8c9OimlpKumiVBYW71qkwY5iEhtUKImkm2WL4dnn/WtZ1On+tJm//5w221w1FGVSpuZLMeFsdWrNmmQg4jUhgaZDkBEklBSAm+/7Udotm8P55zjryBw882waBG8/jocf/wG/c/ilePCbtyMQvYbPYnOV7zGfqMnMW5GYaZDimtE/67kNW5YaVlYBjmISPZSi5pImH3zTUVpc9EiX9o8/XRf2txjj6ijNiNlazkuGzvm18dyr4jUPiVqUm+FdiqF33+vKG2+/74vbR56KNx66walzUSytRwXlo751bmmaCjeQyKSM5IufZrZt2b2lwTbXGBm39Y8LJHaVdZiU1hUjKOixSZj5bXSUn+tzVNO8aXNs8+GX3+F0aNh4UJ4442opc1EsrUcF4aWwNC9R0SkXkqlRa0TkJ9gm3xg62rGIlJnwtJiw7x5vuWsrLTZqhUMHepLm3vumbC0mUi2luPC0BIYmveIiNRr6S59tgDWpnmfkoNqUnZMR8kyoy02v/8Ozz3nE7T33qsobd5yiy9t5qU3GcnGclwYrj4QhlY9EZG4iZqZbVVlUX6UZQAN8df/PAZQ6VPiqklH8XR1Mq/zFpvSUpg82SdnL7wAxcXQtSuMGgWnngodsyuRqm1haAkMQ6ueiEiiFrXvABfx98XBv1gMuLSGMUmOq0lJKV3lqDprsZk/v6K0uXChL22edpovbe61V41Lm7ks0y2BYWjVExFJlKg9hk/UDDgN+ByYGWW7EuBXYKJz7q10Bii5pyYlpXSVo2q1xeaPP2j/+utw7bXwv//5ZOzQQ/2cZwMHpr20KbUjDK16IiJxEzXn3LCy/5vZacBLzrkbajsoyW01KSmlsxyV1hab0lIoKCgvbXZbtQq6dIGbbvKlzS22SM/jSJ3KdKueiEjSgwmcc7qKgaRFTUpKYShHRQ5m2KPkN25Y/indJrzkS5stW8IppzB9l13Y7fzzs660Gdq55URE6ilNeCt1ruzEP3L8FxQVrwOgaePkfgdkuhw1bkYhNz79EX1nT+HY2RPZa9FsSjF+3PtA2o0eDYMGQV4evxcUZGWSlm1XAxARyXUpJWpm1gY4A9gTaI0f7VmVc871S0NskuPWrC8t//9vq9YlnRRkpBxVWgrvvstGl43if7On0GzdGr5t3YF/HXgaL+3UhwZbbcX7Q/rWbUxppnnDRETCJ+lEzcy6AQXApvjBBbG4OOtEgNhJwSXPzOSWCXPDU3L79lt/rc1HH4Xvv2e/Js0Yt2Nvnt/5YKZ37FbeamY5MLeW5g0TEQmfVFrUbgU2A0YDDwCLnHMl8e8iEl28k3/GS24rVsDzz8Mjj8CUKT4ZO/hguOkmBs3L59tVG/4WyYW5tTRvmIhI+KQyQOAA4DXn3FXOue+UpElNJDr5l5Xc6kzZqM1hw/y1Nk8/HZYsgX/+E77/Ht56C046ib8cuWtWXjszGdl6XVARkVyWSouaAXNqKxCpX6KN3qyqTkpuCxZUlDa/+w5atIAhQ3zCtu++GwwIyPRghtqUy89NRCRbpZKoTQP001rSIjIpiFZug1osuZWVNseMgXff9clYv37wj3/A0UdDs2Zx757Lc2vl8nMTEclGqSRqNwATzKy3c66gluKReqQsKag6LQTUQsmttNRfJWDMGH9B9JUrYbvtfHJ26qmwVbRL2OYezZMmIpJdYiZqwZUIqnoZeMvMnsa3sBVFu69z7rG0RCf1Qq2W3BYs8NfZfPRR//8WLeDEE31pc7/9sm6us5rQPGkiItknXovaGDacaqPsrHZq8C/aeoe/RqhI0tJaclu5sqK0WTbxbN++cMMNvrS58cbpeZwso3nSRESyT7xE7fQ6i0KkppyrXNpcsQK23RZuvNGXNrfeOtMRZpzmSRMRyT4xEzXn3KN1GYhItXz3XUVp89tvoXlzOP54X9rcf/96VdpMJN48aeq7JiISTrrQumSflSt9cta3L3TuDNddB506+WVLl8J//wsHHKAkrYpY86T16bYpV744i8KiYhwVfdfGzSjMTKAiIlJOiZpkh7LS5pln+glphw71E9HecIMfJDBxoi9x1tP+Z8kY1LMjowZ3p2N+HgZ0zM9j1ODuTP7q55h910REJLNSudbnt0lsVgr8DnwJvOice6G6gYkAPhl77DHf9+zbb30idvzx/soBKm2mLNqgjeHPzIy6rfquiYhkXirzqDUItu8Q/L0e+BXYJGI/i/HXA+0BnGhmrwODdLkpScnKlfDiiz45mzTJL+vTx5c4Bw/2/dAkbXSNTxGR8Eql9LkLUAj8D9gfaOqc2xxoir8O6P+AH4CO+CsYvAkcDlyczoAlRzkH771XUdo87TRf0rz+en87aZJfpiQt7XSNTxGR8EqlRe2fQCtgP+fc+rKFzrlS4H0zOwT4HPinc+4vZnYc8BVwMvB/aYw5oaJV69hv9CSNYMsGCxdWlDbnz68obZaN2mygbpS1Tdf4TEyjYkUkU1JJ1I4GnopM0iI559aa2SvAEOAvzrlVZjYRODYNcaaksKiY9UEpR7Ovh9CqVZVLm85B795wzTVwzDFqNcsAXeMzNl3RQUQyKZXmik2AJgm2aRxsV2YpqSWDaVHqKl8wQSPYQsA5eP99OOssX9o89VTfgnbddX6QwOTJfiSnkjQJmXhXdBARqW2pJFHfAseY2TXOuT+qrjSzlsAxwIKIxZsDy2oWYnpoBFuGLFwIjz/uW8/mzfOlzeOO86XNAw5QaVNCL6xXdFA5VqR+SOUs+QB+oMBHZnaymXUys7zg9hTgI/yI0PsBzMyA3sDMWDs0s4fN7Cczmx1nm95mNtPMvjCzd1OItxKNYKs7r0ydx/UnXMV7nXpS2qkTXH01dOwIjzziJ6R95BE46CAlaZIVYn13ZPI7pawcq0mKRXJf0i1qzrk7zawrcC7RL7puwAPOuTuDvzcDngbejrPbMcA9MfaHmeUD/wYGOOcWmtlmycTaoMrcWhrBVgecg6lT+e62e+nz2jiOXFvMolbtuGvfE3mtxyFccOah+rUvWWlE/66V+qhB5r9T4pVj9TkTyS0p9R9zzp1vZk8Bw/BzpbXCT3A7A3jMOTclYtsfgSsT7G+KmXWKs8lJ+IlzFwbb/5RMnB3z82iXn6eSQF1YtKiitPnNN7Rr3JTXuu7H89378dGWO+PMt5rpBCLZKoyjYsNajhWR9DNXpeN9nQfgE7VXnXM7R1l3B36Awk5AC+BO51ys1rdzgHMA2rVrt/vYsWNrK+SMWLFiBc1D0tG+werVtH3vPdpPmEDradMw5yjadVeWDhhAwXY9WJcXvSTUvWOrOo40c8J0vCSxbDtec5f+wdqS0g2WN2nYgK7tW2QgorqVbcervtPxSqxPnz7TnHO9oq2r8xGZKWoE7A70A/KAD8zsQ+fc11U3dM49gO9HR69evVzv3r3rMs5aV1BQQEafk3PwwQe+5eyZZ+D332Hrrf2UGqedRv6225IPnDl6UtRZ7jvm53HRyb3rOOjMyfjxyhJh6RCfbcerqMqUIeDLsaMGd6d3PWi5zrbjVd/peNVMzETNzLYK/lvonCuJ+DuhslJlGvwA/OqcWwmsNLMpwK7ABoma1JIffqgobX79NTRrBsce60dtRhkQEMb+PBJOmp/Mq06yGsZyrIjUjngtat8BDtgBnxiV/Z2IS7DfVLwM3GNmjfBzuO0F3J6mfUssxcUwbpxPzt5+27emHXAAXH65n1qjRezSik4gkix1iK9ZsqpJikXqh3gJ1WP4pGt5lb/Txsyexk/h0dbMfgCuw/dJwzn3H+fcl2b2Jv7SVKXAQ865mFN5SA04Bx9+6JOzsWN9aXOrrfzUGqedBtttl/SusvkEEpZSXH2gDvFKVkUksZiJmnNuWLy/08E5NySJbW4Bbkn3Y0ugamkzL6+itNm7d72a60yluLrVIT8van/G+jTnoZJVEUmk/pyFpUJxsW81GzDADwi46ipo1w7++18/Ie1jj0HfvvUqSQNdKqiujejflbzGDSstq2/9GcM4ma6IhEu1+pKZWTd837XmzrnH0xuS1Arn4KOPKkqby5fDllv6JG3o0JRKm7lKrRt1S/0ZNfhGRBJLKVEzsx7AQ0DPiMWPB+sOAt4ATnDOvZKuAKWGCgsrSptz5/rS5jHH+NJmnz71rtUsHpXi6l4292dMByWrIpJI0omamXUBCoCGwJ1AF+CwiE2m4C/AfiygRC2TVq+Gl1/2ydlbb0FpKey/P4wY4UdttmyZ6QhDSa0bkgn1PVkVkfhSaVG7Dj9FRi/n3Bwzu46IRM0558zsA2CPNMcoyXAOPv64orRZVARbbAFXXulLm9tvn+kIQ0+tGyIiEjapJGr98NfdnBNnm0XAITULSVKyeHFFafOrr6Bp08qlzYYNE+2hWnJ1Ggu1boiISJikkqi1xl8pIB7Dt7pJbVq9GsaPh0ceqSht7rcfPPigL222qt1ramoaCxERkbqRSqL2I5BoaOBO+FY1SbfI0ubTT1cubZ52GnTpUmehaJJOyWW52losItkplURtEjDEzLo65zaYWMrM9sCXR+9NV3CCL20+8QR7/Pvf8P33vrQ5eLAvbfbtW2ulzbghaRoLyVFqLRaRsEklURsFHAdMMbORQAcAM9sJOBA/2OAP4NY0x5gzkv6lvno1vPKKbz17800oLWX9TjvBAw/A8cfXemkzEU1jIblKrcUiEjZJJ2rOublmdgzwNHBPsNjw1+E0oAgY7JxbmO4gc0HCX+rOwaefVpQ2f/sNOnb0F0IfOpQZS5bQu3fvzD2BCJrGQnKVWotFJGxSmvDWOfemmXUGhgJ7A5vgL9r+IfCIc25Z+kPMDbF+qT/83FQGvfOdT9DmzPGlzaOP9qXNfv0qSptLltR1yDFpGgvJVWotFpGwSfkSUs65IvyEt3emPZocFvmLvMn6dRw87yOOnfUOBy2YDq4U9tkH7r/flzbz81PadyY6P2saC8lFai0WkbCp1rU+JXUdWjWlzVezOHb2Owyc8y75q1ewpPkmPHnQCZz2n+uga/VOBOr8LJmSi6Mj1VosImETN1Ezs62qs1P1U4uwdCk88QRv/PchWs6fy5qGjZnQZR+e634w07fbjX8e2wO6Vv8koM7Pkgm5/ANBrcUiEiaJWtS+A1yK+3RJ7De3rVlTedRmSQkt996bmSeM5vJGO/D1moZ0yM/jn2n4pa7Oz5IJ+oEgIlI3EiVUC9kwUcsHWgHf10ZAWcs5mD7dJ2dPPQXLlkGHDv5C6EOHQrdu9AAmpPlh1flZMkE/EERE6kbcRM0516nqsmAOtWucc51rKabssnQpPPmkT9Bmz4aNNoJBg/yozUMOqfUJadX5WTJBPxBEROpGg2rcJ9VSaO5ZswZeeAGOPNJfxumvf4WNN4b77vPTaIwdCwMG1MlVAwb17Miowd3pmJ+HAR3z8xg1uLvKT1KrRvTvSl7jyu9v/UAQEUm/+t2XLBXOwYwZ/kLoZaXNzTf3SdrQobDDDhkLTZ2fpa5pdKSISN1QopbIjz9WlDZnzapc2jz4YGikl1DqJ/1AEBGpfcoyolm7Fl591Sdnr78OJSWw557w73/DiSdC69aZjlBERETqASVqZZyDmTN9cvbkk/Drr9C+PVx2mS9t7rhjpiMUERGRekaJ2k8/VZQ2P/8cmjSBgQPh9NP9qE2VNkVERCRDEl2ZoKQa65xzLtzZzdq18NprFaXN9ethjz3g3nt9abNNm0xHKCIiIpKwRc2qsc/q3KduzJixYWlz+HBf2txpp0xHJyIiIlJJoglvqzPPWrj89JOfTmPMGPjss4rS5rBhcOihKm2KiIhIaOVmluIcjBvnk7PXXvOlzV694J57YMgQlTZFREQkK+Rmovb553D00dCuHVxyiS9t7rxzpqMSERERSUluJmrNm8MTT0D//iptioiISNbKzSxm223hiCMyHYWIiIhIjWT/YAERERGRHKVETURERCSklKiJiIiIhJQSNREREZGQUqImIiIiElJK1ERERERCKjen54hi3IxCbpkwl8VFxXTIz2NE/64M6tkx02GJiIiIxFQvErVxMwq58sVZFK8rAaCwqJgrX5wFoGRNREREQqtelD5vmTC3PEkrU7yuhFsmzM1QRCIiIiKJ1YtEbXFRcUrLRURERMKgXpQ+O+TnURglKeuQn5eBaKSm1N9QRETqi3rRojaif1fyGjestCyvcUNG9O+aoYikusr6GxYWFeOo6G84bkZhpkMTERFJu3qRqA3q2ZFRg7vTMT8PAzrm5zFqcHe1wmQh9TcUEZH6pF6UPsEna0rMsp/6G4qISH1SL1rUJHfE6leo/oYiIpKLlKhJVlF/QxERqU/qTelTckNZ+VqjPnOXRvWKiFRQoiZZR/0Nc5euIiIiUplKnyISGhrVKyJSmRI1EQkNjeoVEalMiZqIhIZG9YqIVKZETURCQ6N6RUQq02ACEQkNjeoVEakso4mamT0M/An4yTm3c5T1vYGXgQXBohedczfUWYAiUuc0qldEpEKmW9TGAPcAj8XZ5n/OuT/VTTgiIiIi4ZHRPmrOuSnAskzGICIiIhJW5pzLbABmnYBX45Q+XwB+ABYDf3XOfRFjP+cA5wC0a9du97Fjx9ZSxJmxYsUKmjdvnukwJEk6XtlFxyu76HhlFx2vxPr06TPNOdcr2rqwJ2otgVLn3AozOxy40zm3faJ99urVy3366afpDzaDCgoK6N27d6bDkCTpeGUXHa/souOVXXS8EjOzmIlapvuoxeWc+z3i/6+b2b/NrK1z7pdMxiXhoetCiohILgt1omZm7YEfnXPOzPbE96n7NcNhSUjoupAiIpLrMj09x9NAb6Ctmf0AXAc0BnDO/Qc4FjjPzNYDxcCJLtO1WgmNeNeFVKImIiK5IKOJmnNuSIL19+Cn78hJKtvVjK4LKSIiuU6XkMqQsrJdYVExjoqy3bgZhZkOLWvoupAiIpLrlKhlSLyynSRH14UUEZFcF+rBBLlMZbua03UhRUQk1ylRy5AO+XkURknKVLZLja4LKSIiuUylzwxR2U5EREQSUYtahqhsJyIiIokoUcsgle1EREQkHpU+RUREREJKiZqIiIhISClRExEREQkpJWoiIiIiIaVETURERCSklKiJiIiIhJQSNREREZGQUqImIiIiElJK1ERERERCSomaiIiISEgpURMREREJKSVqIiIiIiGlRE1EREQkpJSoiYiIiISUEjURERGRkFKiJiIiIhJSStREREREQkqJmoiIiEhIKVETERERCSklaiIiIiIhpURNREREJKSUqImIiIiEVE4marMKl7Pf6EmMm1GY6VBEREREqi0nEzWAwqJirnxxlpI1ERERyVo5m6gBFK8r4ZYJczMdhoiIiEi15HSiBrC4qDjTIYiIiIhUS84nah3y8zIdgoiIiEi15HSilte4ISP6d810GCIiIiLV0ijTAdSWjvl5jOjflUE9O2Y6FBEREZFqyclErXvHVrx/Rd9MhyEiIiJSIzld+hQRERHJZkrUREREREJKiZqIiIhISClRExEREQkpJWoiIiIiIaVETURERCSklKiJiIiIhJQSNREREZGQUqImIiIiElJK1ERERERCypxzmY4h7czsZ+D7TMeRZm2BXzIdhCRNxyu76HhlFx2v7KLjldjWzrlNo63IyUQtF5nZp865XpmOQ5Kj45VddLyyi45XdtHxqhmVPkVERERCSomaiIiISEgpUcseD2Q6AEmJjld20fHKLjpe2UXHqwbUR01EREQkpNSiJiIiIhJSStREREREQkqJWsiYWVczmxnx73czu8TM2pjZ22b2TXDbOtOxStzjNdLMCiOWH57pWMUzs+Fm9oWZzTazp82sqZl1NrOPzGyemT1jZk0yHad4MY7XGDNbEPH56pHpOMUzs4uDY/WFmV0SLNP5qwbURy3EzKwhUAjsBVwALHPOjTazK4DWzrnLMxqgVFLleJ0OrHDO3ZrZqCSSmXUE3gN2dM4Vm9mzwOvA4cCLzrmxZvYf4DPn3H2ZjFXiHq/ewKvOueczGZ9UZmY7A2OBPYG1wJvAucA56PxVbWpRC7d+wHzn3PfAQODRYPmjwKBMBSUxRR4vCa9GQJ6ZNQKaAUuAvkDZSV+fr3CperwWZzgeiW0H4CPn3Crn3HrgXWAwOn/ViBK1cDsReDr4fzvn3JLg/0uBdpkJSeKIPF4AF5rZ52b2sJr6w8E5VwjcCizEJ2jLgWlAUXBiAfgB6JiZCCVStOPlnHsrWP3P4PN1u5ltlLEgJdJs4AAz28TMmuFbqrdE568aUaIWUkEfmaOA56quc75erZp1iEQ5XvcB2wI98CeY2zITmUQKEuaBQGegA7AxMCCjQUlM0Y6XmZ0CXAl0A/YA2gAqo4WAc+5L4GbgLXzZcyZQUmUbnb9SpEQtvA4Dpjvnfgz+/tHMNgcIbn/KWGQSTaXj5Zz70TlX4pwrBR7E99mQzDsYWOCc+9k5tw54EdgPyA9KawBb4PsaSuZFO177OueWOG8N8Aj6fIWGc+6/zrndnXMHAr8BX6PzV40oUQuvIVQuo40Hhgb/Hwq8XOcRSTyVjlfZl1LgaHxJQDJvIbC3mTUzM8P3K5wDTAaODbbR5ys8oh2vLyNO+obv76TPV0iY2WbB7Vb4/mlPofNXjWjUZwiZ2cb4L6htnHPLg2WbAM8CWwHfA8c755ZlLkopE+N4PY4vezrgO+DPEX00JIPM7HrgBGA9MAM4C98nbSy+jDYDOCVorZEMi3G83gA2BQxfXjvXObciUzFKBTP7H7AJsA641Dk3UeevmlGiJiIiIhJSKn2KiIiIhJQSNREREZGQUqImIiIiElJK1ERERERCSomaiIiISEgpURORrGdmnczMmdmYNOzrOzP7ruZRpfSYpwfx71lluTOzglp83PFmNj+4soaIhJASNRGpMTPbPUgqPoqxfkiw3plZ5yjr88xstZmtyrbrNtY0STSz5sBNwCvOuY/TGlxi1+Ivz/SXOn5cEUmSEjURSYcZ+MvF7G5mLaOs70fF9f36Rlm/H7AR8F41J5otBHbAXwMy2/wFaA+MrusHds7NxF+T8e/BRbRFJGSUqIlIjQXXNC0AGgIHRdmkb7D+V6InamXLJlbz8dc5577Ktqs/mFlD4Fzga+fc1AyF8SiQD5yUoccXkTiUqIlIupQlWZUSMTPrhC+vTQTeBfpEue8GiZqZNTKz883sQzP7PSiLzjCzC82s0ndXvPKjmXUxsxfM7DczW2lmU83sCDMbFtxnWLQnY2Ybm9ktZrbQzNaY2Twzuzy4vmTZNiOBBcGfQyPKuzH3W8UhwJb4y+skzcxGmFmpmb1vZm0ilrcyszvM7IeglPyVmV1qZtvEKc++DKwGzkwlBhGpG40yHYCI5IxJwW2/Ksv7RaxfDgw2sx2dc3MAglJpL3zpdHqwrDHwCtAfmIu/sPNqfJJ3N7AXcGqigMysGzAVaA28BnwObAO8BLwe566NgQlAB/x1JdfjL/49GmgKXB9sV4BvjboY+AwYF7GPmYniAw4Obt9LYluCBPUO4CLgReBk59zqYF1T/Gu8G74U/STQCvg7cECsfTrnVpvZNPzFz1uVXa9WRMJBiZqIpIVz7kszWwLsbGabOud+Dlb1BVYAnwC/RyybE/z/IHzJtCAooYJPLvoD9wCXOOdKoLxU+ABwhpk975x7OUFY9+KTtPOdc/eVLTSzw4ifqHXAJ16HOOeKg/tcD3wNDDezm4Jya0EwQvRiYKZzbmSCeKraP7j9NNGGQSL2JDAY/7pcHPF6AYzAJ2ljgZNccCFnM/snQQIcxyf4foL7Ef91EZE6ptKniKTTRMCoXN7sA/zPObfeOfcF8BOVy6OVyp5Bq9FFwFJgeFmSBhD8/zL8wIST4wViZlsG+54H3B+5zjn3BvBOgufyl7IkLbjPT/gyYSuga4L7JmsrYJ1z7td4GwXlzXeAo4HLnXMXVUnSAIYCpcCVZUlaEPcifCtcPEsj4hGREFGLmoik0yTgFHyC9KyZ7QBsDtwesU0BcIiZNQiSjar907oAbYBvgKsjuoRFKsaP8oynR3D7QZSkBny58eAoywGWO+fmRVm+KLhtneCxk7UJvuQbTzvgfXzJ9hTn3FNVNwjKx9sCi5xz30XZR6LS6rLgtm2C7USkjilRE5F0Kku2+lW5nRSxTQFwPNDTzBYC3YFC59xXwfpNgtvtgeviPFbzBLG0Cm5/jLE+1nKAohjL1we3DRM8drKK8X3e4mkPtAR+IHbCVTYlSnWeK0BeRDwiEiIqfYpI2jjnFgLzge0iSo9F+M7tZSYHt33xZVGj8rQcZZ3ZX3LOWZx/G0ycW0VZf7h2MdbHWl6XfgJaBoMnYvkMX9bsCEwxs22ibFPT51qWHP+UYDsRqWNK1EQk3cqSroOB3sC7kaXHoOVsKT5RizZ/2lf45G7vBAlMIjOD232qTucR2D/Ksuoo60NXnVa2z4PbuH3enHNPACfiBzlMMbMuVdb/DnwLdAymQ6kq0XPtFtzOTLCdiNQxJWoikm5lZc7h+L5ck6NsMxk/ZcShwd/liZpzbj1+Co7NgbvMLK/qnc1sczPbMV4QQeteAbAd8Ocq9x9A7P5pqfoNP7ihOh3xC4LbvRNt6Jx7HjgW34/sXTPbqcomj+G/00dVmettS+CSBLvfG/gFmJ1U1CJSZ5SoiUi6TcInLt0j/q5qMrAxfiLcuc65wirrbwTG42ft/8bMHjOzUWb2XzObgu+vdVQSsVyAb537t5m9Ymb/NLOng32XTe0RbaBB0pxzK4CPgAPM7Ekzu87MrjazXZK4+8v4Frn+ST7WeGAgvv9dgZntGrH6X/gWsROBaWY22szuC5aV9W3b4LmaWVd8kvli5GhREQkHJWoiklbB/Gmzgj9jtdJEtrJtcNko59w6/ASzp+EnvP0TflqOAfjvrWvwc4olimUOsA9+gtsD8C1LnfDTXJQlL79Hu2+KTsVPqDsAPwDiRvycZoniW4Sf2PdIM0tqJKlzbgJwOH4QwmQz2yNYXkzFhMDt8S2affAXfB8V3D3acx0a3N4XZZ2IZJjpB5SI1Edm9iT++pbdnHNzMxjHvvjpNy51zt2eaPtqPsbZ+ImCz3XO3R+xfCN837YvnXPpKgWLSBopURORnBUMItjMObe0yvJ++EtEzXXOVe3rVefM7FngQGAb59yqGuyng3NucZVlW+FbDzcHto5cb2aXALcBuznnPqvu44pI7dE8aiKSy5oAi8xsMn406XpgJ/zF0Nfi+7CFwV+BM/B99r6owX5eCEbKTsP3zeuELxs3w1+xYHGV7dcAZypJEwkvtaiJSM4Krg16B34akC3wCcsvwBRgtHNuRux7Zx8zOx/fX257/ICDFfg57O5xzr2YydhEpHqUqImIiIiElEZ9ioiIiISUEjURERGRkFKiJiIiIhJSStREREREQkqJmoiIiEhI/T/ggl32GRRkJwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.load('data/weight_height_data.npz')\n",
    "X, y = data['X'], data['y']\n",
    "\n",
    "#Create and estimate the model\n",
    "Xn = np.hstack((np.ones((y.size, 1)), X))\n",
    "beta = inv(Xn.T @ Xn) @ Xn.T @ y\n",
    "y_hat = Xn @ beta\n",
    "mse = np.mean((y_hat - y) ** 2)\n",
    "\n",
    "#Plot scatter model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y)\n",
    "plt.title('Relation between weight and height (in meters)', y=1.05, fontsize=20)\n",
    "plt.xlabel('Weight (kg)', fontsize=20)\n",
    "plt.ylabel('Height (meters)', fontsize=20)\n",
    "\n",
    "#Add model parameters\n",
    "plt.plot([X.min(), X.max()], [Xn.min(axis=0) @ beta, Xn.max(axis=0) @ beta], ls='-', c='r')\n",
    "plt.xlim((X.min(), X.max()))\n",
    "plt.text(70, 1.9, r'$\\hat{\\beta}_{weight} = %.5f$' % beta[1], fontsize=18)\n",
    "plt.text(70, 1.8, r'$MSE = %.5f$' % mse, fontsize=18)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta estimates can only be interpreted in reference to the error and scale.\n",
    "One could intuitively think that this is a quite small beta-parameter, but the beta always have to be interpreted in reference to the Mean Squared Error, which is also quite low. A large beta may not mean anything if the error is very large, and a small beta may be significant, if the error is also small.\n",
    "\n",
    "Now, to illustrate the problem of interpretating 'raw' beta-weights, let's rephrase our objective of predicting height based on weight: we'll try to predict *height in centimeters* based on weight (still in kilos). So, what we'll do is just rescale the data points of $\\mathbf{y}$ (height in meters) so that they reflect height in centimeters. We can simply do this by multipling our $\\mathbf{y}$ by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cm = y * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you wouldn't expect our model to change, right? We only rescaled our target ... As you'll see below, this actually changes a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a67cba1915b72950",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta cm: [[67.88037704]\n",
      " [ 1.28134464]]\n",
      "MSE cm: 44.31965034401705\n"
     ]
    }
   ],
   "source": [
    "\n",
    "beta_cm = inv(Xn.T @ Xn) @ Xn.T @ y_cm\n",
    "y_hat_cm = Xn @ beta_cm\n",
    "mse_cm = np.mean((y_hat_cm - y_cm) ** 2)\n",
    "\n",
    "print(\"Beta cm:\",beta_cm)\n",
    "print(\"MSE cm:\",mse_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you compare the beta-parameters between the two models (one where $y$ is in meters, and one where $y$ is in centimeters), you see a massive difference &mdash; a 100 fold difference to be exact\\*! This is a nice example where you see that the (raw) value of the beta-parameter is completely dependent on the scale of your variables. (Actually, you could either rescale $\\mathbf{X}$ or $\\mathbf{y}$); both will have a similar effect on your estimated beta-parameter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink1</b>: Note that the MSE is a 10,000 times larger in the model with <tt>y_cm</tt> compared to <tt>y</tt> (in meters). From your understanding of how MSE is calculated, do you understand why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your ToThink1 answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is the mean squared difference between the true y and the predicted y. We've scaled the y_hat so it's now 100 times larger than before (the difference is 100), and if we square 100 we have 10,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink2</b>: By now, you know that the scale of the data (either $X$ or $y$) influences the magnitude of the raw parameter estimates. One could argue that this is not relevant for fMRI data because all data (i.e. different voxels in the brain) all measure the same type of signal, so their scale shouldn't differ that much. This, however, is a false assumption.\n",
    "\n",
    "Think of (at least) two reasons why voxels might differ in their scale and write them down in the text cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2602edc5df20bc9f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Write your ToThink2 answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute *t*-values and *p*-values\n",
    "So, you've seen that interpreting beta-parameters by themselves is useless because their value depends very much on the scale of your variables. But how should we, then, interpret the effects of our predictors on our target-variable? From the plots above, you probably guessed already that it has something to do with the MSE of our model (or, more generally, the model fit). That is indeed the case. As you might have noticed, not only the beta-parameters depend on the scale of your data, the errors (residuals) depend on the scale as well. In other words, not only the *effect* (beta-values) but also the *noise* (errors, MSE) depend on the scale of the variables! \n",
    "\n",
    "### *t*-values\n",
    "In fact, the key to getting interpretable effects of our predictors is to divide (\"normalize\") our beta-parameter(s) by some quantity that summarizes how well our model describes the data. This quantity is the **standard error of the beta-parameter**, usually denoted by $\\mathrm{SE}_{\\beta}$. The standard error of the beta-parameter can be computed by taking the square root of the **variance of the beta-parameter**. If we'd divide our beta-estimate with it's standard error, we compute a statistic you are all familiar with: the *t*-statistic! Formally:\n",
    "\n",
    "\\begin{align}\n",
    "t_{\\hat{\\beta}} = \\frac{\\hat{\\beta}}{\\mathrm{SE}_{\\hat{\\beta}}} = \\frac{\\hat{\\beta}}{\\sqrt{\\mathrm{var}(\\hat{\\beta})}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink3</b>: Suppose that I know the $\\mathrm{SE}$ of a particular beta-parameter. How can I derive the variance of that parameter (i.e., how do I go from the $\\mathrm{SE}$ to the variance)? And yes, the answer is as straightforward as you'd think.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2602edc5df20bc9f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Write your ToThink2 answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "square it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Another way to think about it</b> is that the t-value is the \"effect\" ($\\hat{\\beta}$) divided by your (un)certainty or confidence in the effect ($\\mathrm{SE}_{\\hat{\\beta}}$). In a way, you can think of t-values as \"uncertainty-normalized\" effects.\n",
    "\n",
    "So, what drives (statistical) uncertainty about \"effects\" (here: $\\hat{\\beta}$ parameters)? To find out, let's dissect the uncertainty term, $\\mathrm{SE}_{\\hat{\\beta}}$, a little more. The standard error of a parameter can interpreted conceptually as the \"unexplained variance of the model\" (or *noise*) multiplied with the \"design variance\" (or: *the variance of the parameter due to the design*). In this lab, we won't explain what *design variance* means or how to compute this, as this is the topic of the second notebook of this week (`design_of_experiments`).\n",
    "\n",
    "For now, we treat \"design variance\", here, as some known (constant) value given the design matrix ($\\mathbf{X}$). So, with this information, we can construct a conceptual formula for the standard error of our parameter(s):\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{SE}_{\\hat{\\beta}} = \\sqrt{\\mathrm{noise} \\cdot \\mathrm{design\\ variance}}\n",
    "\\end{align}\n",
    "\n",
    "Now we also create a \"conceptual formula\" for the *t*-statistic:\n",
    "\n",
    "\\begin{align}\n",
    "t_{\\hat{\\beta}} = \\frac{\\hat{\\beta}}{\\mathrm{SE}_{\\hat{\\beta}}} = \\frac{\\mathrm{effect}}{\\sqrt{\\mathrm{noise} \\cdot \\mathrm{design\\ variance}}}\n",
    "\\end{align}\n",
    "\n",
    "**This (conceptual) formula involving effects, noise, and design variance is probably the most important concept of this course**. The effects (*t*-values) we measure in GLM analyses of fMRI data depend on two things: the effect measured ($\\hat{\\beta}$) and the (un)certainty of the effect ($SE_{\\hat{\\beta}}$), of which the latter term can be divided into the unexplained variance (\"noise\") and the design variance (uncertainty of the parameter due to the design).\n",
    "\n",
    "These two terms (noise and design variance) will be central to the next couple of weeks of this course. In this week's second notebook (topic: design of experiments), we'll focus on how to optimize our *t*-values by minimizing the \"design variance\" term. Next week (topic: preprocessing), we'll focus on how to (further) optimize our *t*-values by minimizing the error/noise.\n",
    "\n",
    "While we're going to ignore the design variance for now, we are, however, going to learn how to calculate the \"noise\" term.\n",
    "\n",
    "In fact, the noise term is *very* similar to the MSE, but instead of taking the *mean* of the squared residuals, we sum the squared residuals (\"sums of squared erros\", SSE) and divide it by the model's degrees of freedom (DF). People usually use the $\\hat{\\sigma}^{2}$ symbol for this noise term:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{noise} = \\hat{\\sigma}^{2} = \\frac{\\sum_{i=1}^{N}(\\hat{y_{i}} - y_{i})^2}{\\mathrm{df}} \n",
    "\\end{align}\n",
    "\n",
    "where the degrees of freedom (df) are defined as the number of samples ($N$) minus the number of predictors *including the intercept* ($P$):\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{df} = N - P\n",
    "\\end{align}\n",
    "\n",
    "So, the formula of the *t*-statistic becomes:\n",
    "\n",
    "\\begin{align}\n",
    "t_{\\hat{\\beta}} = \\frac{\\hat{\\beta}}{\\sqrt{\\frac{\\sum_{i=1}^{N}(\\hat{y_{i}} - y_{i})^2}{N - P} \\cdot \\mathrm{design\\ variance}}}\n",
    "\\end{align}\n",
    "\n",
    "Alright, enough formulas. Let's see how we can compute these terms in Python. We're going to calculate the *t*-statistic of the weight-predictor for both models (the meter and the centimeter model) to see whether we can show that essentially the (normalized) effect of weight on height in meters is the same as the effect on height in centimeters; in other words, we are going to investigate whether the conversion to *t*-values \"normalizes\" the beta-parameters.\n",
    "\n",
    "First, we'll create a function for you to calculate the design-variance. You *don't* have to understand how this works; we're going to explain this in another tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_variance(X, which_predictor=1):\n",
    "    ''' Returns the design variance of a predictor (or contrast) in X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape (N, P)\n",
    "    which_predictor : int or list/array\n",
    "        The index of the predictor you want the design var from.\n",
    "        Note that 0 refers to the intercept!\n",
    "        Alternatively, \"which_predictor\" can be a contrast-vector\n",
    "        (which will be discussed later this lab).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    des_var : float\n",
    "        Design variance of the specified predictor/contrast from X.\n",
    "    '''\n",
    "    \n",
    "    is_single = isinstance(which_predictor, int)\n",
    "    if is_single:\n",
    "        idx = which_predictor\n",
    "    else:\n",
    "        idx = np.array(which_predictor) != 0\n",
    "    \n",
    "    c = np.zeros(X.shape[1])\n",
    "    c[idx] = 1 if is_single == 1 else which_predictor[idx]\n",
    "    des_var = c.dot(np.linalg.inv(X.T.dot(X))).dot(c.T)\n",
    "    return des_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if you want the design variance of the 'weight' parameter in the varianble `Xn` from before, you do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design variance of weight predictor is: 0.000334 \n"
     ]
    }
   ],
   "source": [
    "# use which_predictor=1, because the weight-column in Xn is at index 1 (index 0 = intercept)\n",
    "design_variance_weight_predictor = design_variance(Xn, which_predictor=1)\n",
    "print(\"Design variance of weight predictor is: %.6f \" % design_variance_weight_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we only need to calculate our noise-term ($\\hat{\\sigma}^2$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degrees of freedom: 98\n",
      "Sigma-hat (noise) is: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Let's just redo the linear regression (for clarity)\n",
    "beta_meter = inv(Xn.T @ Xn) @ Xn.T @ y\n",
    "y_hat_meter = Xn @ beta_meter\n",
    "\n",
    "#How many data points?\n",
    "N = y.size\n",
    "#How many parameters in the model?\n",
    "P = Xn.shape[1]\n",
    "#How many degrees of freedom?\n",
    "df = (N - P)\n",
    "print(\"Degrees of freedom: %i\" % df)\n",
    "sigma_hat = np.sum((y - y_hat_meter) ** 2) / df\n",
    "print(\"Sigma-hat (noise) is: %.3f\" % sigma_hat)\n",
    "design_variance_weight = design_variance(Xn, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the *t*-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The t-value for the weight-parameter (beta = 0.013) is: 10.431\n"
     ]
    }
   ],
   "source": [
    "t_meter = beta_meter[1] / np.sqrt(sigma_hat * design_variance_weight)\n",
    "print(\"The t-value for the weight-parameter (beta = %.3f) is: %.3f\" % (beta_meter[1], t_meter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! There's not much more to calculating *t*-values in linear regression. Now it's up to you to do the same thing and calculate the *t*-value for the model of height in centimeters, and check if it is the same as the *t*-value for the weight parameter in the model with height in meters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo1</b>: Calculate the <em>t</em>-statistic for the beta from the centimeter-model you calculated earlier. Store the value in a new variable named <tt>t_centimeter</tt>. Note: you don't have to calculate the design variance again (because <tt>X</tt> hasn't changed!) &mdash; you can reuse the variable <tt>design_variance_weight</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b502342df415d39",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.43132819]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Implement solution for ToDo1 here\n",
    "beta_cm = inv(Xn.T @ Xn) @ Xn.T @ y_cm\n",
    "y_hat_cm = Xn @ beta_cm\n",
    "\n",
    "sse_cm = np.sum((y_hat_cm-y_cm)**2)\n",
    "se_cm = np.sqrt(sse_cm / df * design_variance_weight)\n",
    "t_centimeter = beta_cm[1] / se_cm\n",
    "print(t_centimeter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-values\n",
    "As you can see, calculating *t*-values solves the \"problem\" of uninterpretable beta-parameters!\n",
    "\n",
    "Now, the last thing you need to know is how to calculate the statistical significance of your *t*-value, or in other words, how you calculate the corresponding *p*-value. You probably remember that the *p*-value corresponds to the area under the curve of a *t*-distribution associated with your observed *t*-value *and more extreme values*: \n",
    "![test](http://www.nku.edu/~statistics/Test_o12.gif)\n",
    "*Image credits: Frank Dietrich and Mike Collins, Northern Kentucky University*\n",
    "\n",
    "The function `stats.t.sf(t_value, df)` from the `scipy` package does exactly this. Importantly, this function *always* returns the right-tailed p-value. For negative t-values, however, you'd want the left-tailed *p*-value. One way to remedy this, is to always pass the absolute value of your *t*-value - `np.abs(t_value)` to the `stats.t.sf()` function. Also, the `stats.t.sf()` function by default returns the one-sided *p*-value. If you'd want the two-sided *p*-value, you can simply multiply the returned *p*-value by two to get the corresponding two-sided *p*-value. \n",
    "\n",
    "Let's see how we'd do that in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value corresponding to t(98) = 10.431 is: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# take the absolute by np.abs(t)\n",
    "p_value = stats.t.sf(np.abs(t_meter), df) * 2 # multiply by two to create a two-tailed p-value\n",
    "print('The p-value corresponding to t(%i) = %.3f is: %.8f' % (df, t_meter, p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrasts\n",
    "We're almost done! We're really at 99% of what you should know about the GLM and fMRI analysis (except for some important caveats that have to do with GLM assumptions, that we'll discuss next week). The only major concept that we need to discuss is **contrasts**. Contrasts are basically follow-up statistical tests of GLM parameters, with which you can implement any (linear) statistical test that you are familiar with. *t*-tests, *F*-tests, ANCOVAs &mdash; they can all be realized with the GLM and the right contrast(s). (Again, if you want to know more about this equivalence between the GLM and common statistical tests, check out this [blog post](https://lindeloev.github.io/tests-as-linear/).) Importantly, the choice of contrast should reflect the hypothesis that you want to test.\n",
    "\n",
    "### *t*-tests\n",
    "T-tests in the GLM can be implemented in two general ways:\n",
    "\n",
    "**1. Using a contrast of a parameters \"against baseline\"**\n",
    "\n",
    "This type of contrast basically tests the hypothesis: \"Does my predictor(s) have *any* effect on my dependent variable?\" In other words, it tests the following hypothesis:\n",
    "* $H_{0}: \\beta = 0$        (our null-hypothesis, i.e. no effect)\n",
    "* $H_{a}: \\beta \\neq 0$     (our two-sided alternative hypothesis, i.e. *some* effect)\n",
    "\n",
    "Note that a directional alternative hypothesis is also possible, i.e., $H_{a}: \\beta > 0$ or $H_{a}: \\beta < 0$.\n",
    "\n",
    "**2. Using a contrast between parameters**\n",
    "\n",
    "This type of contrast basically tests hypotheses such as \"Does predictor 1 have a larger effect on my dependent variable than predictor 2?\". In other words, it tests the following hypothesis:\n",
    "* $H_{0}: \\beta_{1} - \\beta_{2} = 0$ (our null-hypothesis, i.e. there is no difference)\n",
    "* $H_{a}: \\beta_{1} - \\beta_{2} \\neq 0$     (our alternative hypotehsis, i.e. there is some difference)\n",
    "\n",
    "Let's look at an example of how we would evaluate a simple hypothesis that a beta has an *some* effect on the dependent variable. Say we'd have an experimental design with 6 conditions:\n",
    "\n",
    "* condition 1: images of **male** faces with a **happy** expression\n",
    "* condition 2: images of **male** faces with a **sad** expression\n",
    "* condition 3: images of **male** faces with a **neutral** expression\n",
    "* condition 4: images of **female** faces with a **happy** expression\n",
    "* condition 5: images of **female** faces with a **sad** expression\n",
    "* condition 6: images of **female** faces with a **neutral** expression\n",
    "\n",
    "Let's assume we have fMRI data from a run with 100 volumes. We then have a target-signal of shape ($100,$) and a design-matrix (after convolution with a canonical HRF) of shape ($100 \\times 7$) (the first predictor is the intercept!). We load in this data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (100, 7)\n",
      "Shape of y: (100,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/data_contrast_example.npz')\n",
    "X, y = data['X'], data['y']\n",
    "\n",
    "print(\"Shape of X: %s\" % (X.shape,))\n",
    "print(\"Shape of y: %s\" % (y.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing linear regression with these 6 predictors (after convolving the stimulus-onset times with an HRF, etc. etc.), you end up with 7 beta values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas corresonding to our 6 conditions (and intercept):\n",
      "array([ 0.08208567, -0.21982422, -0.16284892,  0.53208935,  0.26214462,\n",
      "        0.38945094,  0.21565532])\n"
     ]
    }
   ],
   "source": [
    "betas = inv(X.T @ X) @ X.T @ y\n",
    "betas = betas.squeeze()  # remove singleton dimension; this is important for later\n",
    "print(\"Betas corresonding to our 6 conditions (and intercept):\\n%r\" % betas.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first beta corresponds to the intercept, the second beta to the male/happy predictor, the third beta to the male/sad predictor, etc. etc. Now, suppose that we'd like to test whether images of male faces with a sad expression have an influence on voxel activity (our dependent variable). \n",
    "\n",
    "The first thing you need to do is extract this particular beta value from the array with beta values (I know this sounds really trivial, but bear with me):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'extracted' beta is -0.163\n"
     ]
    }
   ],
   "source": [
    "beta_male_sad = betas[2]\n",
    "print(\"The 'extracted' beta is %.3f\" % beta_male_sad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neuroimaging analyses, however, this is usually done slightly differently: using **contrast-vectors**. Basically, it specifies your specific hypothesis about your beta(s) of interest in a vector. Before explaining it in more detail, let's look at it in a code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beta-contrast is: -0.163\n"
     ]
    }
   ],
   "source": [
    "# Again, we'd want to test whether the beta of \"male_sad\" is different from 0\n",
    "contrast_vector = np.array([0, 0, 1, 0, 0, 0, 0])\n",
    "contrast = (betas * contrast_vector).sum() # we simply elementwise multiply the contrast-vector with the betas and sum it!\n",
    "print('The beta-contrast is: %.3f' % contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Wow, what a tedious way to just select the third value of the beta-array\", you might think. And, in a way, this is indeed somewhat tedious for a contrast against baseline. But let's look at a case where you would want to investigate whether two betas are different - let's say whether male sad faces have a larger effect on our voxel than male happy faces. Again, you *could* do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between betas: 0.057\n"
     ]
    }
   ],
   "source": [
    "beta_difference = betas[2] - betas[1]\n",
    "print(\"Difference between betas: %.3f\" % beta_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but you could also use a contrast-vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contrast between beta 2 and beta 1 is: 0.057\n",
      "This is exactly the same as beta[2] - beta[1]: 0.057\n"
     ]
    }
   ],
   "source": [
    "contrast_vector = np.array([0, -1, 1, 0, 0, 0, 0])\n",
    "contrast = (betas * contrast_vector).sum()\n",
    "print('The contrast between beta 2 and beta 1 is: %.3f' % contrast)\n",
    "print('This is exactly the same as beta[2] - beta[1]: %.3f' % (betas[2]-betas[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Alright, so using contrast-vectors is just a fancy way of extracting and subtracting betas from each other ...\", you might think. In a way, that's true. But you have to realize that once the hypotheses you want to test become more complicated, using contrast-vectors actually starts to make sense.\n",
    "\n",
    "Let's look at some more elaborate hypotheses. First, let's test whether male faces lead to higher voxel activity than female faces, *regardless of emotion*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male - female contrast (regardless of expression): -0.72\n"
     ]
    }
   ],
   "source": [
    "# male faces > female faces\n",
    "contrast_vector = [0, 1, 1, 1, -1, -1, -1]\n",
    "male_female_contrast = (contrast_vector * betas).sum()\n",
    "print(\"Male - female contrast (regardless of expression): %.2f\" % male_female_contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or whether emotional faces (regardless of *which* exact emotion) lead to higher activity than neutral faces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion - neutral contrast (regardless of which emotion): -1.23\n"
     ]
    }
   ],
   "source": [
    "# Emotion (regardless of which emotion, i.e., regardless of sad/happy) - neutral\n",
    "contrast_vector = np.array([0, 1, 1, -2, 1, 1, -2])\n",
    "emo_neutral_contrast = (contrast_vector * betas).sum()\n",
    "print(\"Emotion - neutral contrast (regardless of which emotion): %.2f\" % emo_neutral_contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how contrast-vectors come in handy when calculating (more intricate) comparisons? In the male-female contrast, for example, instead 'manually' picking out the betas of 'sad_male' and 'happy_male', averaging them, and subtracting their average beta from the average 'female' betas ('happy_female', 'sad_female'), you can \n",
    "specify a contrast-vector, multiply it with your betas, and sum them. That's it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink4</b>: In the last contrast (<tt>emo_neural_contrast</tt>), we set all the \"emotional\" predictors (sad/happy) to 1, but the neutral predictors to minus <em>2</em> ... Why are these set to -2 and not -1? Write your answer below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c9b2ee94e3e03078",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Write your ToThink4 answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want them to cancel out. So when we have four contrasts that we're interested in (the emotional ones, the '1's), those we are not interested in need to be the opposite of 4, that is -4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo2</b>: Create a contrast vector for the hypothesis: sad faces (regardless whether it's male or female) activate this voxel more than neutral faces (regardless of whether it's male/female). Multiply this contrast vector with the betas and store the result in a variable named <tt>contrast_todo</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49f8094366dfb9fa",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.521142649253213\n"
     ]
    }
   ],
   "source": [
    "# Implement the sad - neutral contrast here:\n",
    "contrast_vector = np.array([0,0,1,-1,0,1,-1])\n",
    "contrast_todo = np.sum(contrast_vector * betas) # same as the dot product apparently\n",
    "print(contrast_todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not only telling you about contrasts because we think it's an elegant way of computing beta-comparisons, but also because virtually every major neuroimaging software package uses them, so that you can specify what hypotheses you exactly want to test! You'll also see this when we're going to work with FSL (in week 5) to perform automated whole-brain linear regression analyses.\n",
    "\n",
    "Knowing how contrast-vectors work, we now can extend our formula for *t*-tests of beta-parameters such that they can describe **every possible test** (not only *t*-tests, but also ANOVAs, *F*-tests, etc.) of betas \"against baseline\" or between betas that you can think of: \n",
    "\n",
    "Our \"old\" formula of the *t*-test of a beta-parameter:\n",
    "\\begin{align}\n",
    "t_{\\hat{\\beta}} = \\frac{\\hat{\\beta}_{j}}{\\mathrm{SE}_{\\hat{\\beta}}}\n",
    "\\end{align}\n",
    "\n",
    "And now our \"generalized\" version of the *t*-test of *any* contrast/hypothesis:\n",
    "\n",
    "\\begin{align}\n",
    "t_{\\mathbf{c}\\hat{\\beta}} = \\frac{\\sum_{j=1}^{P}{c_{j}\\hat{\\beta}_{j}}}{\\mathrm{SE}_{\\mathbf{c}\\hat{\\beta}}} \n",
    "\\end{align}\n",
    "\n",
    "in which $\\mathbf{c}$ represents the entire contrast-vector, and $c_{j}$ represents the $j^{\\mathrm{th}}$ value in our contrast vector. By the way, we can simplify the (notation of the) numerator a little bit using some matrix algebra. Remember that multiplying two (equal length) vectors with each other and then summing the values together is the same thing as the (inner) \"dot product\" between the two vectors? \n",
    "\n",
    "This means that you can also evaluate this elementwise multiplication and sum of the contrast-vector and the betas using the dot-product:\n",
    "\n",
    "\\begin{align}\n",
    "t_{\\mathbf{c}\\hat{\\beta}} = \\frac{\\mathbf{c}\\hat{\\beta}}{\\mathrm{SE}_{\\mathbf{c}\\hat{\\beta}}} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you need the contrast vector in the *numerator* of the *t*-value formula (i.e., $\\mathbf{c}\\hat{\\beta}$), but it turns out that you actually also need the contrast-vector in the denominator, because it's part of the calculation of design variance. Again, we will discuss how this works exactly in the next notebook. In the function `design_variance`, it is also possible to calculate design variance for a particular contrast (not just a single predictor) by passing a contrast vector to the `which_predictor` argument.\n",
    "\n",
    "We'll show this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design variance of happy/male - sad/male: 0.024\n"
     ]
    }
   ],
   "source": [
    "# E.g., get design-variance of happy/male - sad/male\n",
    "c_vec = np.array([0, 1, -1, 0, 0, 0, 0])  # our contrast vector!\n",
    "dvar = design_variance(X, which_predictor=c_vec)  # pass c_vec to which_predictor\n",
    "print(\"Design variance of happy/male - sad/male: %.3f\" % dvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Another Example: Sad faces > happy faces </b>\n",
    "    \n",
    "Let's calculate the *t*-value and *p*-value for the hypothesis \"sad faces have a larger effect than happy faces (regardless of gender) on our dependent variable\" (i.e. voxel activity). In other words, test the hypothesis: $\\beta_{sad} - \\beta_{happy} \\neq 0$ (note that this is a two-sided test!).\n",
    "\n",
    "Store the *t*-value and *p*-value in the variables <tt>tval_todo</tt> and <tt>pval_todo</tt> respectively. We reload the variables below (we'll call them <tt>X_new</tt> and <tt>y_new</tt>) to make sure we're working with the correct data. Note that the <tt>X_new</tt> variable already contains an intercept; the other six columns correspond to the different predictors (male/hapy, male/sad, etc.). In summary, we will do the following:\n",
    "\n",
    "- (We don't have to calculate the betas; this has already been done (stored in the variable <tt>betas</tt>)\n",
    "- calculate \"sigma-hat\" ($\\mathrm{SSE} / \\mathrm{df}$)\n",
    "- calculate design-variance (use the <tt>design_variance</tt> function with a proper contrast-vector)\n",
    "- calculate the contrast ($\\mathbf{c}\\hat{\\beta}$)\n",
    "- calculate the t-value and p-value\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-55833a9a2174215c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (100, 7)\n",
      "Shape of y: (100,)\n",
      "t= 1.2645629707362716\n",
      "p= 0.2091879177229414\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/data_contrast_example.npz')\n",
    "X_new, y_new = data['X'], data['y']\n",
    "\n",
    "print(\"Shape of X: %s\" % (X_new.shape,))\n",
    "print(\"Shape of y: %s\" % (y_new.shape,))\n",
    "\n",
    "#The contrast vector for sad>happy\n",
    "cvec = np.array([0, -1, 1, 0, -1, 1, 0])\n",
    "#The design variance for this particular contrast\n",
    "this_dvar = design_variance(X, cvec)\n",
    "#Calculating predicted values\n",
    "y_hat = X_new.dot(betas)\n",
    "\n",
    "\n",
    "this_sse = ((y_new - y_hat) ** 2).sum() / (X.shape[0] - X.shape[1])\n",
    "#t-value\n",
    "tval_SadHap = cvec.dot(betas) / np.sqrt(this_sse * this_dvar)\n",
    "#p-value\n",
    "pval_SadHap = stats.t.sf(np.abs(tval_SadHap), (X_new.shape[0] - X_new.shape[1])) * 2\n",
    "\n",
    "print(\"t=\", tval_SadHap)\n",
    "print(\"p=\",pval_SadHap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *F*-tests on contrasts\n",
    "In the previous section we discussed how to calculate *t*-values for single contrasts. However, sometimes you might have a hypothesis about multiple contrasts at the same time. This may sound weird, but let's consider an experiment.\n",
    "\n",
    "Suppose you have data from an experiment in which you showed images circles which were either blue, red, or green. In that case, you have three predictors. Then, you could have very specific question, like \"Do blue circles activate a voxel significantly compared to baseline\", which corresponds to the following null and alternative hypothesis:\n",
    "\n",
    "* $H_{0}: \\beta_{blue} = 0$ (our null-hypothesis, i.e. there is no activation compared to baseline)\n",
    "* $H_{a}: \\beta_{blue} > 0$   (our alternative hypothesis, i.e. blue activates relative to baseline)\n",
    "\n",
    "However, you can also have a more general question, like \"Does the presentation of *any* circle (regardless of color) activate a voxel compared to baseline?\". This question represents the following null and alternative hypothesis:\n",
    "\n",
    "* $H_{0}: \\beta_{blue} = \\beta_{red} = \\beta_{green} = 0$\n",
    "* $H_{a}: (\\beta_{blue} > 0) \\vee (\\beta_{red} > 0) \\vee (\\beta_{green} > 0)$\n",
    "\n",
    "The $\\vee$ symbol in the alternative hypothesis means \"or\". So the alternative hypothesis nicely illustrates our question: is there *any* condition (circle) that activates a voxel more than baseline? This hypothesis-test might sound familiar, because it encompasses the *F*-test! In other words, an *F*-test tests *a collection of contrasts* together. In the example here, the *F*-test tests the following contrasts together (ignoring the intercept) of our beta-parameters:\n",
    "\n",
    "* `[1, 0, 0]` ($\\mathrm{red} > 0$)\n",
    "* `[0, 1, 0]` ($\\mathrm{blue} > 0$)\n",
    "* `[0, 0, 1]` ($\\mathrm{green} > 0$)\n",
    "\n",
    "Thus, a *F*-test basically tests this contrast-*matrix* all at once! Therefore, the *F*-tests is a type of \"omnibus test\"! \n",
    "\n",
    "Now, let's look at the math behind the *F*-statistic. The *F*-statistic for set of $K$ contrasts (i.e., the number of rows in the contrast-matrix) is defined as follows:\n",
    "\n",
    "\\begin{align}\n",
    "F = (\\mathbf{c}\\hat{\\beta})^{T}[K\\mathbf{c}((X^{T}X)^{-1}\\hat{\\sigma}^{2})\\mathbf{c}^{T}]^{-1}(\\mathbf{c}\\hat{\\beta})\n",
    "\\end{align}\n",
    "\n",
    "With a little imagination, you can see how the *F*-test is an extension of the *t*-test of a single contrast to accomodate testing a set of contrasts together. Don't worry, you don't have to understand how the formula for the *F*-statistic works mathematically and you don't have to implement this in Python. But you *do* need to understand what type of hypothesis an *F*-test tests! \n",
    "\n",
    "Let's practice this in a ToDo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo3</b>\n",
    "    \n",
    "Remember the temporal basis sets from before? Suppose we have an experiment with two conditions (\"A\" and \"B\") and suppose we've created a design matrix based on convolution with a single-gamma basis set (with a canonical HRF, its temporal derivative, and its dispersion derivative). Together with the intercept, the design matrix thus has 7 columns (2 conditions * 3 HRF + intercept).\n",
    "\n",
    "The order of the columns is as follows:\n",
    "* column 1: intercept\n",
    "* column 2: canonical HRF \"A\"\n",
    "* column 3: temporal deriv \"A\"\n",
    "* column 4: dispersion deriv \"A\"\n",
    "* column 5: canonical HRF \"B\"\n",
    "* column 6: temporal deriv \"B\"\n",
    "* column 7: dispersion deriv \"B\"\n",
    "\n",
    "Suppose I want to test whether there is *any* difference in response to condition \"A\" ($A > 0$) compared to baseline, and *I don't care what element of the HRF caused it*. I can use an F-test for this. What would the corresponding contrast-*matrix* (in which each row represents a different contrast) look like? \n",
    "\n",
    "We've created an 'empty' (all-zeros) 2D matrix below with three rows. It's up to you to fill in the matrix such that it can be used to test the above question/hypothesis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-82c295ab029883fe",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill in the correct values!\n",
    "contrast_matrix = np.array([\n",
    "\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "])\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "contrast_matrix = np.array([\n",
    "    [0,1,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Alright, now you know basically everything about how to perform a univariate fMRI analysis! \n",
    "\n",
    "\"Wait, that's it?\", you might ask (or not). Well, yeah, regular univariate analyses as you might read about in scientific journals do basically what you've just learned, but then not on a single voxel, but on each voxel in the brain separately. Basically just a gigantic for-loop across voxels in which everytime the same design ($\\mathbf{X}$) is used to predict a new voxel-signal ($\\mathbf{y}$). Afterwards, the *t*-values of the contrast (hypothesis) you're interested in are plotted back onto a brain, color-code it (high t-values yellow, low t-values red), and voilà, you have your pretty brain plot.\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/archive/4/4a/20171016185711%211206_FMRI.jpg)\n",
    "*Image credits: Wikimedia*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink4</b>: More explained variance (i.e., a smaller \"sums of squared error\" term) does not always mean that your <em>t</em>-value is higher. Explain how this might happen.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-50d9ec0c4060b7ff",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Insert your answer to ToThink4 here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the t-statistics also depend on the design variance.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo4</b>: Suppose that, within the hypothesized face-experiment explained earlier, you want to know which parts of the brain show (significantly) more activity during periods without stimuli (i.e., no faces were shown, i.e., \"rest\") than during periods with stimuli. Define a contrast vector which would test this hypothesis and store it in a variable <tt>cvec_rest</tt>. Remember: the original face experiment had 7 predictors (the first one being the intercept, followed by 6 face predictors).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc092da92c99eae3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement the assignment here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "cvec_rest = np.array([0, -1, -1, -1, -1, -1, -1])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-60d8290f1add9cc2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class='alert alert-success'>\n",
    "    \n",
    "    \n",
    "<b>SOLUTIONS</b>\n",
    "    \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>Solution to ToThink1</b>: Note that the MSE is a 10,000 times larger in the model with <tt>y_cm</tt> compared to <tt>y</tt> (in meters). From your understanding of how MSE is calculated, do you understand why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is squared error. 100 x 100 =10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>Solution for ToThink2</b>:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2602edc5df20bc9f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "1. Inhomogeneity of the signal at some spots (lower signal)\n",
    "2. Type of scanner.\n",
    "3. Different tissue types (white matter, gray matter, CSF, mix)\n",
    "4. Closeness to the headcoil (subcortical structures for example have generally a lower SNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>Solution for ToThink3</b>:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2602edc5df20bc9f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Square it: SE(beta)^2 = var(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>Solution for ToDo1</b>:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b502342df415d39",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "beta_cm = inv(Xn.T @ Xn) @ Xn.T @ y_cm\n",
    "y_hat_cm = Xn @ beta_cm\n",
    "sse_cm = ((y_hat_cm - y_cm) ** 2).sum()\n",
    "se_cm = np.sqrt(sse_cm / df * design_variance_weight)\n",
    "t_centimeter = beta_cm[1] / se_cm\n",
    "print(\"t-value for centimeter model:\", t_centimeter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>Solution for ToThink4</b>:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c9b2ee94e3e03078",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "They have to sum to 0. If you'd use -1, you would \"weigh\" the emotional predictors twice as heavy as the neutral predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>Solution for ToDo2</b>:.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49f8094366dfb9fa",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.521142649253213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cvec = np.array([0, 0, 1, -1, 0, 1, -1])\n",
    "contrast_todo = cvec @ betas\n",
    "print(contrast_todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>Solution for ToDo3:</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-82c295ab029883fe",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill in the correct values!\n",
    "contrast_matrix = np.array([\n",
    "\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "])\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "contrast_matrix = np.array([\n",
    "    \n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>Solution for ToThink4</b>:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-50d9ec0c4060b7ff",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The noise term of the *t*-value is the sum of squared errors *divided by the degrees of freedom*. So with fewer degrees of freedom, everything else being equal, the noise term will increase. So in situations with (a lot of) predictors that explain little to no variance, including those might actually increase the noise term and the decrease *t*-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>Solution for ToDo4</b>:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc092da92c99eae3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement the assignment here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# We are looking for areas that have more signal when there is not a face stimulus. We can find this by multiplying the face predictors with -1.\n",
    "\n",
    "cvec_rest = np.array([0, -1, -1, -1, -1, -1, -1])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-60d8290f1add9cc2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class='alert alert-success'>\n",
    "    \n",
    "    \n",
    "<b>END OF SOLUTIONS</b>\n",
    "    \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
